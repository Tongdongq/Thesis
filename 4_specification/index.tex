\documentclass[../main/thesis.tex]{subfiles}

\begin{document}

\chapter{Specification}
\ifdefined\main
\acresetall
MAIN IS TRUE
\newcommand{\codePath}{../4_specification/code/}
\newcommand{\figPath}{../4_specification/figures/}
\else
MAIN IS NOT TRUE
\input{../notmain.tex}
\fi

\section{Profiling}

MCprof \cite{mcprof1}\cite{mcprof2} was used to profile Daligner, the results are shown in Figures \ref{prof1} and \ref{prof2}.
The algorithm consists of three main phases: seeding, filtering and aligning.
The seeding phase takes part in the lex\_sort(), count\_thread() and merge\_thread() functions.
The filter implementation is not very complex, and is combined with the alignment part in report\_thread(), this function starts the actual alignment in the form of Local\_Alignment() when a suitable seed is found.
Each seed is aligned in forward and reverse direction.


\todo{insert pictures of profiling, or maybe switch to gprof, because mcprof pictures were off}
% \label{prof1}, \label{prof2}


Since Local\_Alignment() is the most timeconsuming function, that will be the first one the be considered for acceleration.
Each Local\_Alignment() call originates from a particular readpair $A, B$, so executing multiple Local\_Alignment() instances from different readpairs causes no datahazards.
This is why Daligner uses multiple threads, each one running the filter-align function report\_thread() on its own list of readpairs.


\section{Statistics}
Numerous ... have been measured to characterize the process performed by Daligner.
An arbitrary 130MB dataset from the Human 54x dataset from PacBio has been used.
\todo{add citation for 54x dataset}
Tables \ref{tbl:41} to \ref{tbl:4x} show the results.
The 999th permille indicates that the value is lower than or equal to the listed value for 99.9\% of the observed values.

\begin{table}[h]
\caption{Width of the d-wave}
\centering
\vspace{-5pt}
\begin{tabular}{c|c c}
& max & median \\ \hline
forward & 190 & 25 \\
reverse & 189 & 29 \\ \hline
forward* & 13522 & 429 \\
reverse* & 13530 & 215 \\
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Length of snake}
\centering
\vspace{-5pt}
\begin{tabular}{c|c c c c}
& max & median & 75th percentile & 999th permille \\ \hline
forward & 87 & 2 & 0 & 10 \\
reverse & 78 & 2 & 0 & 10 \\
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Number of Kmer matches per readpair}
\centering
\vspace{-5pt}
\begin{tabular}{c|c c c}
max & median & 99th percentile \\ \hline
32733 & 5 & 40 \\
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Max drift (|initial diag - observed diag|)}
\centering
\vspace{-5pt}
\begin{tabular}{c|c c c c}
& max & median \\ \hline
forward & 1938 & 41 \\
reverse & 1323 & 37 \\
\end{tabular}
\end{table}

% 'apos > lasta[diag]'
\begin{table}[h]
\caption{Number of skippable LAcalls per performed LAcall}
\centering
\vspace{-5pt}
\begin{tabular}{c|c c c c}
max & median & 90th percentile & 99th percentile \\ \hline
18384 & 2 & 15 & 70 \\
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Length of overlap}
\centering
\vspace{-5pt}
\begin{tabular}{c|c c c c}
max & median & 75th percentile & 90th percentile & 99th percentile \\ \hline
a & 18384 & 2 & 15 & 70 \\
b & & & &  \\
\end{tabular}
\end{table}







\section{Acceleration}
\subsection{Daligner}
The individual snakes from a d-wave are independent, they can be calculated in parallel.
However, the width of a trimmed d-wave is usually lower than 32, this means a warp cannot be fully utilised if it is assigned a single readpair.
An option is to assign two readpairs to a block, each readpair would then have 16 threads.
This means that the half-warp is fully utilised more often, but when the width of the d-wave is larger than 16, there well be idle threads.

For this reason, coarse-grained parallelism is chosen to be implemented.
The alignment and filter functions are seperated, instead, filter() produces a large list of seed hits (LAcalls), which should be send to the local alignment function.
An advantage of having alignment and filtering intertwined is that Kmer matches that are included in previous alignments can be skipped, Figure \ref{fig:skip} illustrates this.
This advantage is lost when generated the large list, but can be reimplemented using a scheduler that keeps track of the furthest position that was reached in an alignment for each readpair.
Each Kmer match that has enough neighbouring Kmer matches constitutes its own LAcall in the large list, but these are filtered out later.
Each GPU thread is assigned a readpair, and each readpair can have multiple LAcalls.
The host side will assign an LAcall to each GPU thread (unless there are not enough readpairs).
The GPU performs the local alignment, and returns the results.
If a readpair has no unskippable LAcalls left, its GPU thread is assigned a new readpair.

\todo{figure on skipping seed hits 'apos > lasta[diag]'}


% multiple implementations:
% - (V)MTHAB
% - coalesce_diags
% - no_pebble
% - cache_diags
% - xor to find snake
% - cache_vars
% - compress_diags
% - remove_m
% - compress_abseq
% - coalesce_abseq



\subsection{Darwin}
\todo{maybe refer to pseudocode in lstlisting}
It is possible to run the whole GACT kernel on the GPU, for both left and right extension.
However, since it is not known how long the resulting alignment will be, and all GPU threads have to wait until all threads are done, this will cause lots of idle time.
Instead, it is chosen to only have a single tile aligned per GPU-thread per GPU-invocation.
The scheduling and preperation of the tiles will be done by the CPU.
The CPU will receive a large list with seed hit positions, for each hit, the GACT algorithm is performed.
It is not necessary to implement skipping seeds/Kmer matches for Darwin, since it generates less redundant seeds than Daligner.



It is also possible to implement a more fine-grained parallelism, and assign multiple threads to the same tile using wavefront parallelism \todo{cite}
The tilesize of 320 should be enough to allow for enough utilization.
This method uses less memory, since the Smith-Waterman matrices and the sequences are shared.
However, this complicates the implementation, since the number of elements on an anti-diagonal is often not equal to 32.
Inter-thread communication or scheduling is needed to efficiently use the hardware.
This is not the case when using coarse-grained parallelism.
Since the tiles are almost always the same size (it only differs when aligning the end of a sequence), the exact operations that need to be executed are the same for all threads.
This requires enough seed hits to keep every processing element busy, but the 130MB dataset produces two lists of 400k seeds using default settings. 
Besides, the memory used by a single thread is not very large, since the tiles that need to be aligned are usually at most 320 bases.

GACT is designed to allow for an affine gap penalty, so it uses three matrices to keep track of the scores.
The whole tile is calculated per column, so only two columns need to be kept from the matrices: the current and the previous.
This means the implementation uses $O(T)$ memory for the score, where T is the tilesize.
The traceback pointers need to be kept for the whole tile, this takes $O(T^2)$.
Since the matrices and traceback pointers are read/written for every score element, it makes sense to coalesce these, to optimize the memory bandwidth.

GASAL \cite{GASAL} provides an API for GPU accelerated functions that perform different types of genomic alignment, such as local, global and semi-global.
It can perform one-to-one, one-to-many and all-to-all alignment.
GASAL first packs the bases in a 4-bit format, where 8 bases are packed into a 32-bit integer.
The integers are padded with 'N' nucleotides, these do not affect the score when aligned.
The packed bases do not have to be copied back to the CPU, because they are used during alignment.
The second phase is the actual alignment, since there are 8 bases in one integer, the matrix is divided into 8x8 tiles.
Figures \ref{GASAL1} and \ref{GASAL2} show the order of the tiles and elements in which they are calculated.


\figC{width=.7\textwidth , clip, trim=70 490 260 120}{GASAL.pdf}{Illustration of GASAL algorithm, high level.}{fig:GASAL1}
\figC{width=.4\textwidth , clip, trim=70 240 400 400}{GASAL.pdf}{Illustration of GASAL algorithm, within an 8-by-8 tile.}{fig:GASAL2}

Instead of keeping two whole columns of the matrices in memory, it keeps two rows of 8 elements (the width of a tile) as working memory.
This way, a whole column of 8-by-8 tiles can be calculated.
Because the elements of the next column need the scores of the last, an array of scores is kept for the vertical column.
It does not provide traceback information, when the start position of an alignment is requested, it performs the alignment again, but in reverse.

GASAL needs to be adapted to perform actual traceback, to guarantee that the tiles of GACT have a minimum overlap.
It is chosen to perform the actual traceback on the GPU, instead of copying all the pointers to the CPU, and doing the traceback there.
\todo{this choice might seem odd, since traceback on GPU is usually very divergent, might need more explanation}







\end{document}










