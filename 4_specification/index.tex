\documentclass[../main/thesis.tex]{subfiles}

\begin{document}

\chapter{Specification}
\ifdefined\main
\acresetall
MAIN IS TRUE
\newcommand{\codePath}{../4_specification/code/}
\newcommand{\figPath}{../4_specification/figures/}
\else
MAIN IS NOT TRUE
\input{../notmain.tex}
\fi

\section{Profiling}

MCprof \cite{mcprof1}\cite{mcprof2} was used to profile Daligner, the results are shown in Figures \ref{prof1} and \ref{prof2}.
The algorithm consists of three main phases: seeding, filtering and aligning.
The seeding phase takes part in the lex\_sort(), count\_thread() and merge\_thread() functions.
The filter implementation is not very complex, and is combined with the alignment part in report\_thread(), this function starts the actual alignment in the form of Local\_Alignment() when a suitable seed is found.
Each seed is aligned in forward and reverse direction.


\todo{insert pictures of profiling, or maybe switch to gprof, because mcprof pictures were off}
% \label{prof1}, \label{prof2}


Since Local\_Alignment() is the most timeconsuming function, that will be the first one the be considered for acceleration.
Each Local\_Alignment() call originates from a particular readpair $A, B$, so executing multiple Local\_Alignment() instances from different readpairs causes no datahazards.
This is why Daligner uses multiple threads, each one running the filter-align function report\_thread() on its own list of readpairs.


\section{Statistics}
Numerous ... have been measured to characterize the process performed by Daligner.
An arbitrary 130MB dataset from the Human 54x dataset from PacBio has been used.
\todo{add citation for 54x dataset}
Tables \ref{tbl:41} to \ref{tbl:4x} show the results.
A column with 75\% indicates that 75\% of the observed values is less than or equal to the listed value.

\begin{table}[h]
\caption{Width of the d-wave, the last two rows indicate the width if it was not trimmed}
\centering
\vspace{-5pt}
\begin{tabular}{c|c c}
& max & median \\ \hline
forward & 190 & 24 \\
reverse & 189 & 28 \\ \hline
forward* & 13522 & 428 \\
reverse* & 13530 & 214 \\
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Length of snake}
\centering
\vspace{-5pt}
\begin{tabular}{c|c c c c c c}
& max & median & 75\% & 90\% & 99\% & 99.9\% \\ \hline
forward & 87 & 0 & 0 & 1 & 5 & 9 \\
reverse & 78 & 0 & 0 & 1 & 5 & 9 \\
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Number of Kmer matches per readpair}
\centering
\vspace{-5pt}
\begin{tabular}{c|c c c c}
max & median & 75\% & 99\% \\ \hline
32733 & 4 & 6 & 34 \\
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Max drift (|initial diag - observed diag|)}
\centering
\vspace{-5pt}
\begin{tabular}{c|c c c c}
& max & median \\ \hline
forward & 1938 & 41 \\
reverse & 1323 & 37 \\
\end{tabular}
\end{table}

% 'apos > lasta[diag]'
\begin{table}[h]
\caption{Number of skippable LAcalls per performed LAcall}
\centering
\vspace{-5pt}
\begin{tabular}{c|c c c c}
max & median & 90\% & 99\% \\ \hline
18384 & 2 & 15 & 70 \\
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Length of overlap}
\centering
\vspace{-5pt}
\begin{tabular}{c|c c c c c c}
& max & median & 75\% & 90\% & 99\% & 99.9\% \\ \hline
a & 27240 & 627 & 1341 & 2493 & 6380 & 10726 \\
b & 26465 & 629 & 1345 & 2502 & 6394 & 10755 \\
\end{tabular}
\end{table}

Benchmarks show that not trimming the width of the wave slows down the runtime by about 8x.





\section{Acceleration}
\subsection{Daligner}
The individual snakes from a d-wave are independent, they can be calculated in parallel.
However, the width of a trimmed d-wave is usually lower than 32, this means a warp cannot be fully utilised if it is assigned a single readpair.
An option is to assign two readpairs to a block, each readpair would then have 16 threads.
This means that the half-warp is fully utilised more often, but when the width of the d-wave is larger than 16, there well be idle threads.

For this reason, coarse-grained parallelism is chosen to be implemented.
The alignment and filter functions are seperated, instead, filter() produces a large list of seed hits (LAcalls), which should be send to the local alignment function.
An advantage of having alignment and filtering intertwined is that Kmer matches that are included in previous alignments can be skipped, Figure \ref{fig:skip} illustrates this.
This advantage is lost when generated the large list, but can be reimplemented using a scheduler that keeps track of the furthest position that was reached in an alignment for each readpair.
Each Kmer match that has enough neighbouring Kmer matches constitutes its own LAcall in the large list, but these are filtered out later.
Each GPU thread is assigned a readpair, and each readpair can have multiple LAcalls.
The host side will assign an LAcall to each GPU thread (unless there are not enough readpairs).
The GPU performs the local alignment, and returns the results.
If a readpair has no unskippable LAcalls left, its GPU thread is assigned a new readpair.

\todo{figure on skipping seed hits 'apos > lasta[diag]'}


% multiple implementations:
% - (V)MTHAB
% - coalesce_diags (V,M,H arrays)
% - no_pebble
% - cache_diags
% - xor to find snake
% - cache_vars
% - compress_diags
% - remove_m
% - compress_abseq
% - coalesce_abseq

\paragraph{Coalesce main data arrays}
The main data of a wave is stored in a few arrays.
These arrays are not interleaved when used by the CPU, since each thread has its own cache.
On the GPU, they need to be interleaved to allow for coalesced reads.

\paragraph{No pebbles}
Pebbles are checkpoints in the alignment to allow for easier recalculation of the full alignment when needed.
Shorter alignments do not need these, but long reads create long alignments.
However, since Falcon does not use them \cite{Falcon}, they could be removed.
This means some arrays and loops can be removed.

\paragraph{Cache variables}
The alignment kernel of Daligner is quite complex and contains many variables.
This results in a large register usage per thread.
To try and combat this, a few often used variables can be placed in the unused shared memory.

% V: furthest reaching anti-diagonal in current wave, higher is better
% M: number of matches in current 60 bit history
% H: index of previous Pebble in Pebble-array
% N: indicates next position in aread (i*s) where the next Pebble should be created
\paragraph{Encoding main data arrays}
Each variable in a main data array is a 32-bit integer.
Not all of these values need to use all of those bits.
Table \ref{tbl:value_range} shows the possible values that the arrays can hold.
Figure \ref{fig:CWORK} shows the encoding.
Care must be taken considering the signs of the values.
H only contains non-negative value and -1.
When placed in the least-significant (LS) part and read by masking out the most-significant part (MS), the -1 would give a positive value.
Since checking each time for -1 is inefficient, it could be put in the MS part.
However, N contains negative values as well, it starts at -s (s is 100 by default).
Storing N in the LS part would mean reading and writing a negative value that is unknown at compile time.
It it easier to give H an offset of 1, so that the stored value is always non-negative.
Since N can still be negative, it is put in the MS part, where shifting right is possible for both positive and negative values.
M is always non-negative, but V can contain -1, therefore V is put in the MS part, and M in the LS part.
T contains a bitvector that represents the history of the last C (default = 60) bases, based on this, the number of matches is updated.
T cannot be easily reduced in size without reducing C, although M can be removed by calculating the popcount of T when it is needed.

\begin{table}
\centering
\begin{tabular}{c|c c}
& min & max \\ \hline
V & -1 & 63225 \\
M & 0 & 61 \\
H & -1 & 4677 \\
N & -100 & 34500 \\
\end{tabular}
\end{table}

\figC{width=.7\textwidth , clip, trim=60 490 210 120}{CWORK.pdf}{Encoding of data.}{fig:CWORK}

\paragraph{Remove M}
The M array is stored in global memory, so reading and writing is relatively expensive.
Its only purpose is to require a certain number of matches in the last C (60) match/mismatches, but it is only checked if the V value is larger than the highest V value for the current wave.
Since it is used infrequently, an option is to calculate the M when it is needed, based on the T vector.
This can be done using the popcount instruction, which is available as an intrinsic instruction in CUDA (\_\_popc()) \cite{CUDA_math}. 


\paragraph{XOR to find snakelength}
The CPU version uses one byte per base, and each base is retrieved on its own.
Since each base only needs two bits, four bases can be encoded into one byte, or sixteen bases into one integer.
The actual comparison is done via the XOR operation, matching bases result in zeros.
By counting the leading zeros via the intrinsic \_\_clz() \cite{CUDA_math} and dividing by two, the snakelength can be determined.
If the result contains 32 zeros, the next integers must also be checked.
The position that needs to be checked might not be divisible by eight, in this case, the integers that hold the encoded bases must be shifted and masked to prevent comparison between sequences of unequal length.
Another option is to have 16 copies of the encoded bases, so that there is always an integer available that starts at the correct position, eliminating the need for shifting and masking, but it costs 16x more memory, and shifts and masks are relatively cheap on a GPU.
The T vector is filled with the appropiate amount of ones afterwards.



\subsubsection{Splitting the alignment between GPU and CPU}
There are other ways of using the GPU, the implementations GPU\_SINGLE, GPU\_SNAKE and COMPRESS\_ABSEQ try to do that.
GPU\_SINGLE lets the GPU compute a large bitmatrix, where 0 means a mismatch, and 1 means a match.
These bits will be read by the CPU, instead of having the CPU do the comparisons itself.
GPU\_SNAKE computes the whole snake on the GPU, and stores it at some index that is based on the positions in reads a and b.
When the CPU wants to know the length of a snake during alignment, it only has to look up the length in the array.
COMPRESS\_ABSEQ calculates the XORred bitvector between integers with compressed bases like described above, but the results returned to the CPU.
During alignment on the CPU, it only has to read the correct bitvector, and count the leading zeros.


\subsection{Darwin}
\todo{maybe refer to pseudocode in lstlisting}
It is possible to run the whole GACT kernel on the GPU, for both left and right extension.
However, since it is not known how long the resulting alignment will be, and all GPU threads have to wait until all threads are done, this will cause lots of idle time.
Instead, it is chosen to only have a single tile aligned per GPU-thread per GPU-invocation.
The scheduling and preperation of the tiles will be done by the CPU.
Tracking the full alignment in two strings is also done on the CPU, they are used to calculate the total alignment score.
The CPU will receive a large list with seed hit positions, for each hit, the GACT algorithm is performed.
It is not necessary to implement skipping seeds/Kmer matches for Darwin, since it generates less redundant seeds than Daligner.



It is also possible to implement a more fine-grained parallelism, and assign multiple threads to the same tile using wavefront parallelism \todo{cite}
The tilesize of 320 should be enough to allow for enough utilization.
This method uses less memory, since the Smith-Waterman matrices and the sequences are shared.
However, this complicates the implementation, since the number of elements on an anti-diagonal is often not equal to 32.
Inter-thread communication or scheduling is needed to efficiently use the hardware.
This is not the case when using coarse-grained parallelism.
Since the tiles are almost always the same size (it only differs when aligning the end of a sequence), the exact operations that need to be executed are the same for all threads.
This requires enough seed hits to keep every processing element busy, but the 130MB dataset produces two lists of 400k seeds using default settings. 
Besides, the memory used by a single thread is not very large, since the tiles that need to be aligned are usually at most 320 bases.

GACT is designed to allow for an affine gap penalty, so it uses three matrices to keep track of the scores.
The whole tile is calculated per column, so only two columns need to be kept from the matrices: the current and the previous.
This means the implementation uses $O(T)$ memory for the score, where T is the tilesize.
The traceback pointers need to be kept for the whole tile, this takes $O(T^2)$.
Since the matrices and traceback pointers are read/written for every score element, it makes sense to coalesce these, to optimize the memory bandwidth.

GASAL \cite{GASAL} provides an API for GPU accelerated functions that perform different types of genomic alignment, such as local, global and semi-global.
It can perform one-to-one, one-to-many and all-to-all alignment.
GASAL first packs the bases in a 4-bit format, where 8 bases are packed into a 32-bit integer.
The integers are padded with 'N' nucleotides, these do not affect the score when aligned.
The packed bases do not have to be copied back to the CPU, because they are used during alignment.
The second phase is the actual alignment, since there are 8 bases in one integer, the matrix is divided into 8x8 tiles.
Figures \ref{GASAL1} and \ref{GASAL2} show the order of the tiles and elements in which they are calculated.


\figC{width=.7\textwidth , clip, trim=70 490 260 120}{GASAL.pdf}{Illustration of GASAL algorithm, high level.}{fig:GASAL1}
\figC{width=.4\textwidth , clip, trim=70 240 400 400}{GASAL.pdf}{Illustration of GASAL algorithm, within an 8-by-8 tile.}{fig:GASAL2}

Instead of keeping two whole columns of the matrices in memory, it keeps two rows of 8 elements (the width of a tile) as working memory.
This way, a whole column of 8-by-8 tiles can be calculated.
Because the elements of the next column need the scores of the last, an array of scores is kept for the vertical column.
It does not provide traceback information, when the start position of an alignment is requested, it performs the alignment again, but in reverse.

GASAL needs to be adapted to perform actual traceback, to guarantee that the tiles of GACT have a minimum overlap.
It is chosen to perform the actual traceback on the GPU, instead of copying all the pointers to the CPU, and doing the traceback there.
\todo{this choice might seem odd, since traceback on GPU is usually very divergent, might need more explanation}

Since the order of accesses is slightly different between the CPU version and GASAL, the reported coordinates of the element with the maximum score could differ.

% CUDA streams
% CMAT
% CBASES (for non GASAL)
% CPBASES (for GASAL)


\paragraph{No score}
If the score of the alignment is not needed, the used traceback pointers do not have to returned to the CPU.
It also saves the time spend on building the aligned strings.

\paragraph{Coalesce matrices}
The score matrices in GASAL are reduced to fit in the registers, and cannot be coalesced.
There are two main matrices left: the global array that stores the scores of one column, and the tracebackpointer matrix.
The global array actually stores the scores of the three matrices needed to allow for an affine gap penalty, coalescing them means that they have to be seperated.

\paragraph{Coalesce packed bases}
The bases are uninterleaved when they are send to the GPU, and after packing.
This means that during alignment, they cause uncoalesced reads.
To prevent this, the writing of the packed bases can interleave them, but this means the packing kernel has write in an uncoalesced manner.
When preparing the bases on the CPU, they are interleaved, so that the packing kernel can read and write coalesced, and the alignment kernel can read coalesced too.








\end{document}










