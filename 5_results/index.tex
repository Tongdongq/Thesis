\documentclass[../thesis.tex]{subfiles}

\begin{document}

\chapter{Results}
\ifdefined\main
%\acresetall
\newcommand{\codePath}{5_results/code/}
\newcommand{\figPath}{5_results/figures/}
\else
\input{../notmain.tex}
\fi

\section{Hardware}
Tests on two systems were done: TACC \cite{TACC} and ce-cuda.

A TACC node (S824L) has two 10-core 3.42 GHz POWER8 CPUs \cite{datasheet_TACC}, 256 GB RAM and a Tesla K40m.
The CUDA version for TACC is 7.5, and the operating system is Ubuntu 3.19.0-28-generic.
The GCC version is 4.9.2.

ce-cuda has 2 Intel Xeon E5-2620 @ 2.4 GHz CPUs \cite{datasheet_ce_cuda}, 32 GB RAM and a Tesla K40c.
The clocks on the K40 were 745 MHz and 3004 MHz for core and memory respectively.
The CUDA version for ce-cuda is 9.2, and the operating system is CentOS 7.5.
The GCC version is 4.8.5.

The K40c and K40m have the same performance, the only difference lies in their cooling method \cite{K40m}.

\section{Daligner}
\subsection{Runtimes}
The different runtime configurations are indicated with A B C, where A is the number of CPU threads, B is the number of GPU blocks that each CPU thread launches, and C is the number of GPU threads in each block.
In Tables \ref{tbl:daligner1} to \ref{tbl:daligner4}, the reverse complement of the second database is not used, because the 3000 bp custom PacBio and uniform reads are generated without complemented reads.
Table \ref{tbl:daligner1} shows the CPU results.
Note that B30 and NP change the output.
Using B30 does not make a large difference, it makes sense, since a CPU thread uses a 32-bit register, instead of 64-bit register, per diagonal.
Registers may not be cheap for CPUs, but the data is cached, so it is likely loaded as fast as without B30.
NP reduces the runtime significantly for all implementations, which is as expected, since creating Pebbles takes both time and memory.
What is interesting is that CABSEQ increases the runtime for all CPU configurations.
The instructions to count leading/trailing zeros, and popcount are likely causes.
The number of instructions is also much higher, but it reduces the amount of branches.
The GPU is more likely to benefit from this tradeoff, since it is worse at executing branch instructions than the CPU.



%./generate.sh 4 3000 100000 5000 4
\begin{table}
\centering
\caption{Runtimes for different Daligner CPU implementations, with 3000 bp custom PacBio reads, run on TACC, A vs B.}
\label{tbl:daligner1}
\begin{tabular}{c c c c}
implementation & number of threads & runtime (s) & runtime with NP (s) \\ \hline
baseline & 1 & 124.1 & 92.0 \\
& 8 & 22.7 & 17.3 \\
& 32 & 14.6 & 11.8 \\ \hline
B30 & 1 & 124.2 & 92.5 \\
& 8 & 22.7 & 17.4 \\
& 32 & 14.4 & 11.6 \\ \hline
CABSEQ & 1 & 132 & 89.1 \\
& 8 & 32.1 & 17.2 \\
& 32 & 15.4 & 12.4 \\
\end{tabular}
\end{table}

Table \ref{tbl:daligner2} shows runtimes for GPU implementations, which have the same output as the original.
All listed GPU configurations use STREAM.
CPU runtimes have their run configuration listed as 'TX', where X is the number of CPU threads.
Running the fastest configuration (CWORK CABSEQ) with 1 256 64 results in a runtime of 82.9, which is faster than the single-threaded CPU implementation.
However, running single-threaded is not a realistic scenario on this hardware.
Caching compressed bases (CAABSEQ) really slows down the program, it is possible that calculating the addresses in shared memory takes a long time.
Additionally, for each base, a check must be done to see if it is cached or not.
CAWORK is also slower, there are more registers needed per thread, according to the Occupancy Calculator \cite{occupancy_calculator}, this decreases the maximum number of active warps per SMX from 20 to 16.


\begin{table}
\centering
\caption{Runtimes for different Daligner optimizations, with 3000 bp custom PacBio reads, run with 8 64 64 on TACC, A vs B.}
\label{tbl:daligner2}
\begin{tabular}{l c}
& runtime (s) \\ \hline
CPU T1 & 124 \\
CPU T8 & 22.6 \\
CPU T32 & 14.6 \\ \hline
base GPU & 54.8 \\ 
CABSEQ & 53.1 \\
CWORK & 48.7 \\
CWORK CABSEQ & \textbf{46.6} \\
CWORK CABSEQ RM & 46.7 \\
CWORK CABSEQ WORK & 46.8 \\
%CABSEQ CAABSEQ & 68.1 \\
CWORK CABSEQ CAABSEQ & 58.7 \\
CWORK CABSEQ CAWORK & 48.5 \\
\end{tabular}
\end{table}

Table \ref{tbl:daligner3} shows different output altering implementations.
All listed GPU configurations use STREAM.
The configurations with DIAGSX use 8 64 32 as run configuration, since it allows for more active warps per SMX.
When tracking 60 columns, DIAGS5 is the fastest DIAGS implementation, when tracking 30 (B30), the optimum shifts towards DIAGS9.
DIAGSX and CWORK are not used together explicitly, but since V and M are encoded into one integer, and RM is used, CWORK is not needed.

\begin{table}
\centering
\caption{Runtimes for different Daligner optimizations, with 3000 bp custom PacBio reads, run with 8 64 64 on TACC, A vs B, note that B30 and NP change the output.}
\label{tbl:daligner3}
\begin{tabular}{l c}
& runtime (s) \\ \hline
CPU T8 B30 & 22.7 \\
%B30 & 55.0 \\
%B30 RM & 49.7 \\
%RM & 51.0 \\
%B30 CABSEQ & 54.6 \\
%B30 CWORK & 48.3 \\
B30 CWORK CABSEQ & 46.1 \\ \hline
CPU T8 NP & 17.3 \\
%RM CWORK & 49.3 \\ \hline
RM NP CWORK CABSEQ WORK & 27.2 \\
RM NP CWORK CABSEQ WORK B30 & 27.2 \\
%CWORK CABSEQ WORK & 47.3 \\
%CABSEQ WORK NP & 32.1 \\
%CABSEQ WORK & 49.5 \\
%CABSEQ WORK DIAGS5 & 55.2 \\
%CABSEQ WORK DIAGS9 & 62 \\
%CABSEQ WORK DIAGS17 & 145 \\
RM NP CABSEQ WORK & 27.9 \\
RM NP CABSEQ WORK B30 & 26.7 \\
DIAGS5 RM NP CABSEQ WORK & 25.0 \\
DIAGS9 RM NP CABSEQ WORK & 25.4 \\
DIAGS17 RM NP CABSEQ WORK & 33.3 \\
DIAGS5 RM NP CABSEQ WORK B30 & 24.2 \\
DIAGS9 RM NP CABSEQ WORK B30 & 23.8 \\
DIAGS17 RM NP CABSEQ WORK B30 & 28.1 \\
\end{tabular}
\end{table}

Table \ref{tbl:daligner4} shows the effect of some optimizations on different data.
Uniform reads are specifically designed to remove all divergence, the read pairs have seeds and snakes at the same points as other read pairs.

\begin{table}
\centering
\caption{Divergence counteracts the effect of optimizations, all configurations use STREAM.}
\label{tbl:daligner4}
\begin{tabular}{c|c|c}
type of reads & implementation & runtime (s) \\ \hline
uniform reads & base & 35 \\
& WORK CWORK CABSEQ RM NP & 6 \\ \hline
3000 bp custom PacBio reads & base & 20 \\
& WORK CWORK CABSEQ RM NP & 9 \\
\end{tabular}
\end{table}


Replacing 'if(\_\_clz(a1) == 32)\{abpos -= max;\}' with inline PTX gave a 4\% speedup.
Other inline PTX implementations did not give a consistent speedup.

Of the implementations that try changing the algorithm to split the work between CPU and GPU, only GPU\_SINGLE was actually implemented.
It showed a slowdown between 1x and 3x, for various datasets.
An advantage of using the GPU like this, is that there is very little divergence (if the read lengths are similar for all threads).
However, it computes every match/mismatch is the edit graph, while the original CPU implementation only computes the ones it needs.

\newpage

\subsection{Profiling}
nvprof \cite{nvprof} was used to profile the experiments.

% cat 4 reads
For 3000 bp reads from a custom PacBio generator:
\begin{itemize}
\item CWORK reduces the L2 cache misses by 40\%, increases the global load throughput by 35\%, and the number of global load transactions by 12\%. \vspace{-10pt}
\item CABSEQ reduces the number of global load transactions by 15\%, and increases the global load efficiency from 11.8\% to 14.2\%.
\end{itemize}

% cat 1 reads
For 10000 bp uniform reads:
\begin{itemize}
\item DIAGS5 reduces global memory traffic by 25-30\%, and increases the number of executed instructions by 35-40\%. \vspace{-10pt}
\item B30 reduced global memory traffic by 28\%. \vspace{-10pt}
\item ABSEQ reduced the number of global load transactions by 80\%, increased the efficiency of global loads from 41\% to 91\%, reduced the number of L2 cache reads by 60\%, and L1 global loads by 80\%.
\end{itemize}

For uniform reads, the optimizations can reach global read and store efficiencies of 92\% and 96\% respectively, for fixed length simulated PacBio reads, this effect diminishes greatly.
The result of the divergence can also be observed in the runtimes in Table \ref{tbl:daligner4}

\newpage

\section{Darwin}
\subsection{Runtimes}
Darwin has the same run configuration, A B C, where A is the number of CPU threads, B is the number of GPU blocks, and C is the number of threads in a block.
Table \ref{tbl:darwin1} shows the runtimes for CPU baseline with 8 threads, and incremental GPU configurations, for a small dataset.
Tables \ref{tbl:darwin2} and \ref{tbl:darwin3} show similar results for a larger, 50MB dataset.
Note that the speedup for TACC is measured against 64 CPU threads, and for ce-cuda against 8 CPU threads.
The configuration 'GPU GASAL CPBASES' has the same output as the CPU baseline, when using NOSCORE, the output differs slightly.
All configurations use STREAM if applicable.
Table \ref{tbl:darwin8} shows that the speedup holds for scoring schemes that favour gaps, and for schemes that favour mismatches/substitutions.
The scoring scheme is encoded as follows: (match, mismatch, gap\_open, gap\_extend).
The third scoring scheme is based on the default one from EMBOSS Water tool \cite{emboss}.

\begin{table}
\centering
\caption{Runtimes for different Darwin implementations, with 3000 bp custom PacBio reads, run with 8 32 64 on TACC, A vs B.}
\label{tbl:darwin0}
\begin{tabular}{c c c}
implementation & runtime (s) & speedup vs CPU 64 \\ \hline
CPU 64 & 25m47 & -\\
GASAL CMAT CPBASES & 93.7 & 16.5 \\
\end{tabular}
\end{table}

\todo{consider reporting in seconds, vs 25m47}

\begin{table}
\centering
\caption{Runtimes and speedup for different implementations, run on a 2x E.coli dataset with 8 16 32 on TACC, A vs A.}
\label{tbl:darwin1}
\begin{tabular}{c c c c c}
implementation & runtime (s) & speedup vs CPU 8 \\ \hline
CPU 8 & 82 & -\\
BATCH & 87 & 0.94 \\
GPU & 279 & 0.29 \\
GPU CBASES & 269 & 0.30 \\
GPU CMAT & 83 & 0.99 \\
GPU CMAT CBASES & 82 & 1.00 \\
GASAL & 22 & 3.7 \\
GASAL CMAT & 10 & 8.2 \\
GASAL CMAT CPBASES & 11 & 7.5 \\
\end{tabular}
\end{table}

\begin{table}
\centering
\caption{Runtimes and speedup for different implementations, N = 800, run on the 50MB dataset with 8 32 64 on TACC, A vs A.}
\label{tbl:darwin2}
\begin{tabular}{c c c c c}
implementation & runtime & speedup vs CPU 64 \\ \hline
CPU 8 threads & 64m35 & - \\
CPU 32 threads & 29m54 & - \\
CPU 64 threads & 27m56 & - \\
BATCH 32 8 64 & 37m17 & 0.75 \\
GPU CMAT & 14m20 & 1.95 \\
GPU CMAT CBASES & 11m28 & 2.44 \\
GASAL & 11m40 & 2.39 \\
GASAL CMAT & 1m17 & 21.8 \\
GASAL CMAT CPBASES & 1m9 & 23.0 \\
GASAL CMAT CPBASES NOSCORE & 58s0 & 28.9 \\
\end{tabular}
\end{table}

\figC{width=.9\textwidth ,clip,trim=10 260 40 260}{darwin_runtime_bar.pdf}{Runtimes and speedup for different implementations, exact runtimes are shown in Table \ref{tbl:darwin2}, N = 800, run on the 50MB dataset with 8 32 64 on TACC, A vs A.}{fig:darwin_bar1}

\begin{table}
\centering
\caption{Runtimes and speedup for different implementations, N = 800, run on the 50MB dataset with 8 32 64 on ce-cuda, A vs A.}
\label{tbl:darwin3}
\begin{tabular}{c c c c c}
implementation & runtime & speedup vs CPU 8 \\ \hline
CPU 8 threads & 138m21 & - \\
GPU CMAT CBASES & 11m26 & 12.1 \\
GASAL & 11m46 & 11.8 \\
GASAL CMAT & 1m19 & 105 \\
GASAL CMAT CPBASES & 1m16 & 109 \\
GASAL CMAT CPBASES NOSCORE & 56s1 & 148 \\
\end{tabular}
\end{table}

\figC{width=.9\textwidth ,clip,trim=10 260 40 260}{darwin_runtime_bar2.pdf}{Runtimes and speedup for different implementations, exact runtimes are shown in Table \ref{tbl:darwin3}, N = 800, run on the 50MB dataset with 8 32 64 on ce-cuda, A vs A.}{fig:darwin_bar2}

\begin{table}
\centering
\caption{Runtimes and speedup for different scoring schemes, N = 800, run on the 50MB dataset with 8 32 64 on TACC, A vs A.}
\label{tbl:darwin8}
\begin{tabular}{c||c|c|c}
& (2, -1, -2, -2) & (1, -3, -1, -1) & (5, -4, -10, -1) \\ \hline
CPU 64 & 31m15 & 21m28 & 31m27 \\ \hline
GASAL CMAT CPBASES & 76.0s & 59.3s & 78.0s \\
speedup vs CPU 64 & 24.7 & 21.7 & 24.2 \\ \hline
GASAL CMAT CPBASES NOSCORE & 63.5s & 48.5s & 64.5s \\
speedup vs CPU 64 & 29.5 & 26.6 & 29.3 \\
\end{tabular}
\end{table}




%TODO add final profiling, identify bottleneck to make recommendations in Future Work




Table \ref{tbl:darwin6} show different runtime configurations, 8 32 64 is clearly the best one.

\begin{table}
\centering
\caption{Runtimes for different Darwin run configurations, 50MB dataset, de novo, A vs A.}
\label{tbl:darwin6}
\begin{tabular}{c|cc}
Run configuration & runtime TACC (s) \\ \hline
 8 32 64 & 1m9 \\
16 16 64 & 1m32 \\
16  8 64 & 2m6 \\
 4 32 64 & 1m19 \\
 4 64 64 & 1m17 \\
 1 64 64 & 2m25 \\
1 128 64 & 2m14 \\
1 256 64 & 2m28 \\
 32 8 64 & 3m8 \\
\end{tabular}
\end{table}

%TODO add ce-cuda runtimes as well?




\newpage



\subsection{Profiling}
%global load efficiency and stuff

\begin{table}
\centering
\caption{Profile data by nvprof and runtimes for different optimizations, run on the 50MB dataset.}
\label{tbl:darwin_prof}
\begin{tabular}{c c c}
optimizations & avg global load efficiency & runtime (s) \\ \hline
baseline & 12\% & 84.5 \\
GLOBAL & 49\% & 99.2 \\
GLOBAL CPBASES & 73\% & 85.2 \\
CPBASES & 45\% & 82.3 \\
\end{tabular}
\end{table}

Coalescing of the global array and coalescing of the packed bases did improve the global load efficiency, as shown in Table \ref{tbl:darwin_prof}.
The profiling was done using 1 64 64, the runtimes were measured with 8 32 64.
Despite having a higher global load efficiency, GLOBAL does not benefit the runtime, with and without CPBASES.
It is possible that computing the correct addresses for GLOBAL (3 times) causes this effect.
Since it is known when and where the global array is accesses the short3 structure, it is possible the compiler optimizes the accesses.

The alignment kernel consists of two distinct phases: alignment and traceback.
The kernel was split into two kernels, and their runtimes measured, the alignment part takes 96\% of the time.
This makes optimizing the traceback part not efficient.

\begin{table}
\centering
\caption{CPBASES saves some time in alignment, but adds preparation time.}
\label{tbl:CPBASES}
\begin{tabular}{c c c c}
& preparation (s) & packing (ms) & aligning (s) \\ \hline
base & 4.2 & 54 & 41.1 \\
CPBASES & 6.0 & 96 & 38.9 \\
\end{tabular}
\end{table}

The optimization CPBASES coalesces the packed GASAL bases.
This requires a more complex preparation on the GPU size, which interleaves the unpacked bases.
For the 50MB dataset, ran with 1 256 64, the timing results are listed in Table \ref{tbl:CPBASES}.
The preparation and packing take more time, but this is compensated by a faster alignment phase.

Using inline PTX did not give consistent speedup for either implementation.
This should not be a large surprise, modern compilers are quite good at creating efficient low-level code.

As shown in Table \ref{tbl:darwin7}, during runs on the 130MB dataset, the majority of the time was spent on GACT.
This shows that despite large speedups, the bottleneck is still extending the seeds using GACT.
For the 1 128 64 run, 55\% of the GACT time is spent on the CPU, preparing the new GPU arguments, and handling finished alignments.
In the 8 32 64 run, this percentage drops to 15\%.
\begin{table}
\centering
\caption{Percentage spent in GACT, 130MB, A vs A, N = 800, w = 4.}
\label{tbl:darwin7}
\begin{tabular}{c|c c c}
run configuration & total runtime (s) & GACT time (s) & percentage GACT \\ \hline
8 32 64 & 157 & 136 & 87 \\
1 128 64 & 378 & 328 & 87 \\
\end{tabular}
\end{table}


A common performance metric for Smith-Waterman based solutions is CUPS, which stands for Cell Updates Per Second, where a cell is a score element in the matrix.
For 10x de novo A vs A, the Darwin GPU implementation peaked at 11.1 GCUPS.
It did up to 13.3 GCUPS during the 130MB dataset, A vs A, and 13.5 for A vs B.

According to nvprof, the achieved occupancy for a 1 128 64 run of the 50MB dataset is 0.23, this is the ratio of active warps per cycle to the maximum number of warps per SMX, note that the maximum occupancy is 0.50, due to the amount of registers.
A higher occupancy does not automatically a lower runtime \cite{low_occupancy}.
The stall reasons include execution dependencies (40\%), memory dependencies (19\%) and memory throttle (14\%).
The utilization of the arithmetic functional units was 5, on a scale of 1 to 10.
These numbers show there is still potential for speedup on this GPU.
Execution dependencies cause stalls because an input for the instruction is not yet available, this effect can be reduced by utilizing more instruction level parallelism (ILP).
This means the code to calculate one Smith-Waterman element must be rewritten, since that is the computational bottleneck.
However, modern day compilers are quite good at detecting and using ILP, so performance gains might be small.



\section{Sensitivity and specificity}
The algorithms should ultimately be used to find overlaps, the sensitivity and specificity indicate the quality of the report output.
Sensitivity and specificity are defined as:
$$\text{Sensivitity} = \frac{TP}{TP + FN}$$
$$\text{Specificity} = \frac{TP}{TP + FP}$$
where TP, FN and FP are the number of true positives, false negatives and false positives respectively.
Sensitivity indicates how many of the overlaps that the aligner was supposed to find, were actually found.
Specificity indicates how many of the reported overlaps were actually real overlaps.
To boost sensitivity, if an overlap between A and B was found (de novo), the overlap between B and A is also assumed to be found.

PBSIM \cite{PBSIM} is used to generate synthetic PacBio datasets.
The only parameters that is not default is the accuracy, which is set to 0.85\%, to mimic the 15\% error rate of PacBio.
The distribution is also changed: 1.5\% substitution, 9.0\% insertion, 4.5\% deletion.
The reads are generated from an E.coli reference.

For reference-based alignment (mapping), a read that is aligned to the reference within 50 bases of the original location is a true positive.
For de novo based alignment, a true positive must have an overlap of 1000 bases according to PBSIM, and be found by the aligner.
The run configuration of Darwin consists of three numbers, the first denotes the number of CPU threads, the second the number of GPU blocks that each CPU thread launches, the third the number of GPU threads in each of those GPU blocks.
Since Daligner is faster of CPU than on GPU, the CPU version is used, and the number indicates the number of CPU threads.
For Darwin, h (default:21) indicates the number of non-overlapping bases that the Kmers must have to constitute a seed, n (1400) is the number of seeds that are considered for that read pair, and w (4) is the size of the window for the minimizers.
The overlap length threshold is set to 990 during de novo alignment.
For Daligner, h (35) means the same, l (1000) is the minimum length that an overlap must have to be reported.
Both aligners can use a minimum score threshold to further filter the found overlaps.
This will be denoted by s.
The scoring scheme for Darwin is: match = 1, mismatch = -1, gap\_open = -1, gap\_extend = -1, unless noted otherwise.
Daligner does not actually calculate the score, instead the score is defined here as the length of the overlap in a, minus the reported number of differences.

Darwin is not designed to compare two different files against each other, but a particular file against itself.
This will be denoted by 'A vs A', if two different files are compared against each other, this is 'A vs B'.

%%% ref
% daligner

\figC{width=.8\textwidth ,clip,trim=100 240 100 240}{daligner_sens_spec_l_ref.pdf}{Sensitivity vs specificity tradeoff depends on the setting of l.}{fig:daligner1}

% darwin

\figC{width=.8\textwidth ,clip,trim=100 240 100 240}{darwin_N_runtime_ref.pdf}{Sensitivity and runtime depend on number of seeds (N), w = 1.}{fig:darwin1}

\figC{width=.8\textwidth ,clip,trim=100 240 100 240}{darwin_sens_S_ref.pdf}{Sensitivity as a function of score threshold (s) and number of seeds (n), the runtimes are shown in Table \ref{tbl:darwin3}, w = 1.}{fig:darwin2}

\figC{width=.8\textwidth ,clip,trim=100 240 100 240}{darwin_spec_S_ref.pdf}{Specificity as a function of score threshold (s) and number of seeds (n), the runtimes are shown in Table \ref{tbl:darwin3}, w = 1.}{fig:darwin3}

%%% de novo
% daligner

\figC{width=.8\textwidth ,clip,trim=100 240 100 240}{daligner_sens_S_denovo.pdf}{Sensitivity for different Daligner options, their runtimes are listed in Table \ref{tbl:daligner3}}{fig:daligner2}

\figC{width=.8\textwidth ,clip,trim=100 240 100 240}{daligner_spec_S_denovo.pdf}{Specificity for different Daligner options, their runtimes are listed in Table \ref{tbl:daligner3}}{fig:daligner3}

% darwin

\figC{width=.8\textwidth ,clip,trim=100 240 100 240}{darwin_sens_runtime_w_denovo.pdf}{Runtime and sensitivity for different window sizes (w).}{fig:darwin4}

\figC{width=.8\textwidth ,clip,trim=100 240 100 240}{darwin_sens_S_denovo.pdf}{Sensitivity as a function of score threshold (s) and number of seeds (n), w = 4.}{fig:darwin5}

\figC{width=.8\textwidth ,clip,trim=100 240 100 240}{darwin_spec_S_denovo.pdf}{Specificity as a function of score threshold (s) and number of seeds (n), w = 4.}{fig:darwin6}


Figure \ref{fig:daligner1} shows the clear tradeoff between sensitivity and specificity for reference-based alignment.
The exact sensitivities for l200 and l100 are 99.97\% and 100.00\% respectively.
PBSIM generates reads with a minimum size of 100, this is why lowering the default minimum overlap length of 1000bp increases the sensitivity in reference-based alignment.
If the input data is longer than that, or not aligning those shorter reads is acceptable, the setting for l can be higher.
Figures \ref{fig:darwin2} and \ref{fig:darwin3} show a similar tradeoff between sensitivity and specificity for Darwin.
While the default value for the number of seeds is 1400, this number can be lowered to 400 without noticeable impact on the sensitivity.
When going lower than 400, the sensitivity is reduced, but the runtime is also lower.
The runtime and number of seeds have an affine relation, so trading runtime for sensitivity is easy.
The specificity is higher when the number of seeds is low, it makes sense that producing less GACTcalls by reducing the number of seeds in the seedposition table reduces the number of false positives.
However, the N = 1400 and 800 options have a higher specificity when also using a high score threshold.
A tradeoff between sensitivity and specificity can also be observed from the figures.

Figures \ref{fig:daligner2} and \ref{fig:daligner3} show the same tradeoff as the others.
The score thresholds below 200 give the same result, that is why part of the x-axis is missing.
The output reacts quite strongly when changing the score threshold from 220 to 260.
Larger Kmers result in lower sensitivity, and higher specificity, as can be expected.


Figure \ref{fig:darwin4} combines the sensitivity and runtime, for different windows sizes.
The sensitivity shows a peak when window size is four, so this is the default size for the de novo experiments.

For Figures \ref{fig:darwin5} and \ref{fig:darwin6}, the sensitivity remains constant between score thresholds 0 and 600, so part of the x-axis is missing again.
The exact threshold where the sensitivity starts dropping will depend on the scoring scheme.
It makes sense that there is such a threshold, since the theoretical overlaps have a minimum length of 1000 bp, which should produce a certain score given the scoring scheme and the distribution of errors.

Tables \ref{tbl:daligner5} and \ref{tbl:darwin4} show the runtime, sensitivity and specificity for different parameters.
For both Daligner and Darwin holds: a faster run on TACC is also a faster run on ce-cuda.
However, Daligner runs faster on TACC, while Darwin is faster on ce-cuda.
The Xeon E5-2620 v3 has a maximum of 59 GB/s memory bandwidth, 256 KB L2 cache per core, and 15 MB L3 cache.
The S824L has a maximum of 192 GB/s memory bandwidth per socket, 512 KB L2 cache per core, 8 MB L3-cache per core, and 16 MB L4 cache per DIMM.
Darwin used 18 GB of RAM for a 10x E.coli dataset, run with 8 32 64, Daligner uses only 3 GB of RAM.
One of the differences for Daligner between TACC and ce-cuda is the time to sort the lists.
Sorting on TACC is about 7x faster than on ce-cuda, the alignment phase is also slightly faster.
Since Daligner is designed with caching behaviour in mind, it is possible that the larger caches of S824L benefit Daligner greatly.
From these tables, it is clear that Daligner on CPU is more sensitive, more specific and faster on both platforms than the GPU accelerated Darwin.
The only drawback of Daligner is that is does not actually calculate the optimal overlap, whereas Darwin is empirically shown to be optimal (equal to Smith-Waterman) for certain values of T and O \cite{Darwin2}.
Darwin also uses the traceback pointers to calculate the fully aligned strings, like in Figure \ref{fig:global_vs_local_alignment_fig}.
The output can be changed easily to produce MAF files \cite{MAF}, this is not possible for Daligner.
An explanation for the difference in runtime is the number of cells the algorithms have to process.
Darwin creates 320x320 tiles, which contain 102400 elements, and have an overlap of at least 120 bp with the previous tile.
Daligner follows snakes, and only stops to store values at the end of a snake.
The median wave width is 26, if every snake in the wave had a length of 0, it would mean Daligner does about 8\% of the equivalent cell updates.

Daligner and Darwin also differ in output quality, while Daligner is more sensitive and specific, Darwin has a better resistance against large length thresholds.
This means that Darwin generally outputs longer overlaps.
The sensitivity for various length thresholds are shown in Table \ref{tbl:length_thresholds}.

\begin{table}
\centering
\caption{Runtimes for different Daligner parameters, compiled with NP, one CPU thread, 10x E.coli, de novo, A vs A, score threshold = 200.}
\label{tbl:daligner5}
\begin{tabular}{c|c c c c}
parameters & runtime TACC (s) & runtime ce-cuda (s) & sensitivity & specificity \\ \hline
default & 19.2 & 32.5 & 99.78 & 86.7 \\
h45     & 18.5 & 31.5 & 99.58 & 86.8 \\
k15     & 17.8 & 31.6 & 98.9 & 87.1 \\
k16     & 16.9 & 29.4 & 96.9 & 87.5 \\
\end{tabular}
\end{table}

\begin{table}
\centering
\caption{Runtimes for different Darwin parameters, 1 64 64, 10x E.coli, de novo, A vs A. Sensitivity and specificity are measured using thresholds of 600 and 990 for score and length respectively.}
\label{tbl:darwin4}
\begin{tabular}{c|c c c c}
number of seeds (N) & runtime TACC (s) & runtime ce-cuda (s) & sensitivity & specificity \\ \hline
1400 & 105.6 & 62.8 & 99.33 & 85.0 \\
1100 & 94.2 & 56.1 & 98.9 & 85.4 \\
800 & 89.4 & 48.5 & 97.8 & 86.2 \\
\end{tabular}
\end{table}

\begin{table}
\centering
\caption{Sensitivity for Daligner and Darwin for different length thresholds, the score thresholds are set to 200 and 600 for Daligner and Darwin respectively, Darwin uses N = 800, 10x E.coli, de novo, A vs A.}
\label{tbl:length_thresholds}
\begin{tabular}{c|c c}
 & \multicolumn{2}{c}{sensitivity} \\
length threshold & Daligner & Darwin \\ \hline
990 & 99.78 & 97.8 \\
1100 & 95.4 & 95.0 \\
1200 & 87.9 & 90.2 \\
1500 & 68.3 & 76.1 \\
\end{tabular}
\end{table}






\end{document}










