\documentclass[../main/thesis.tex]{subfiles}

\begin{document}

\chapter{Results}
\ifdefined\main
%\acresetall
\newcommand{\codePath}{../5_results/code/}
\newcommand{\figPath}{../5_results/figures/}
\else
\input{../notmain.tex}
\fi

\section{Hardware}
Tests on two systems were done: TACC and ce-cuda.

TACC nodes have an IBM Power8 S824L, 256 GB RAM and a Tesla K40m.
The CUDA version for TACC is 7.5, and the operating system Ubuntu 3.19.0-28-generic.

ce-cuda has an Intel Xeon E5-2620 @ 2.4 GHz, 32 GB RAM and a Tesla K40c.
The clocks on the K40 were 745 MHz and 3004 MHz for core and memory respectively.
The CUDA version for ce-cuda is 9.2.

The K40c and K40m have the same performance, the only difference lies in their cooling solution \cite{K40m}.

\section{Daligner}
\subsection{Runtimes}
The different runtime configurations are indicated with A B C, where A is the number of CPU threads, B the number of GPU blocks that each CPU thread launches, and C the number of GPU threads in each block.
Table \ref{tbl:daligner1} shows the CPU results.
Note that B30 and NP change the output.
Using B30 does not make a large difference, it makes sense, since a CPU thread uses a 32-bit register, instead of 64-bit register, per diagonal.
Registers may not be cheap for CPUs, but the data is cached, so it is likely loaded as fast as without B30.
NP reduces the runtime significantly for all implementations, which is as expected, since creating Pebbles takes both time and memory.
What is interesting is that CABSEQ increases the runtime for all run configurations.
The instructions to count leading/trailing zeros, and popcount are likely causes.
The number of instructions is also much higher, but it reduces the amount of branches.
The GPU is more likely to benefit from this tradeoff, since it is worse at executing branch instructions than the CPU.



%./generate.sh 4 3000 100000 5000 4
\begin{table}
\centering
\caption{Runtimes for different Daligner CPU implementations, with 3000 bp custom PacBio reads, run on TACC, A vs B.}
\label{tbl:daligner1}
\begin{tabular}{c c c c}
implementation & number of threads & runtime (s) & runtime with NP (s) \\ \hline
baseline & 1 & 124.1 & 92.0 \\
& 8 & 22.7 & 17.3 \\
& 32 & 14.6 & 11.8 \\ \hline
B30 & 1 & 124.2 & 92.5 \\
& 8 & 22.7 & 17.4 \\
& 32 & 14.4 & 11.6 \\ \hline
CABSEQ & 1 & 132 & 89.1 \\
& 8 & 32.1 & 17.2 \\
& 32 & 15.4 & 12.4 \\
\end{tabular}
\end{table}

Table \ref{tbl:daligner2} shows runtimes for GPU implementations, which have the same output as the original.
All listed GPU configurations use STREAM.
Running the fastest configuration (CWORK CABSEQ) with 1 256 64 results in a runtime of 82.9, which is faster than the singlethreaded CPU implementation.
However, running singlethreaded is not a realistic scenario on this hardware.

\begin{table}
\centering
\caption{Runtimes for different Daligner optimizations, with 3000 bp custom PacBio reads, run with 8 64 64 on TACC, A vs B.}
\label{tbl:daligner2}
\begin{tabular}{l c}
& runtime (s) \\ \hline
CPU 1 & 124 \\
CPU 8 & 22.6 \\
CPU 32 & 14.6 \\
base GPU & 54.8 \\ 
CABSEQ & 53.1 \\
CWORK & 48.7 \\
CWORK CABSEQ & 46.6 \\
CABSEQ CAABSEQ & 68.1 \\
CABSEQ CAABSEQ CWORK & 58.7 \\ \hline
\end{tabular}
\end{table}


\begin{table}
\centering
\caption{Runtimes for different Daligner optimizations, with 3000 bp custom PacBio reads, run with 8 64 64 on TACC, A vs B, note that B30 and NP change the output.}
\label{tbl:daligner3}
\begin{tabular}{l c}
& runtime (s) \\ \hline
CPU B30 & \\
B30 STREAM & 55.0 \\
B30 RM STREAM & 49.7 \\
RM STREAM & 51.0 \\
%B30 CABSEQ STREAM & 54.6 \\
%B30 CWORK STREAM & 48.3 \\
B30 CWORK CABSEQ STREAM & 46.1 \\ \hline
%RM CWORK STREAM & 49.3 \\ \hline
RM NP CWORK CABSEQ STREAM WORK & 27.8 \\
RM NP CWORK CABSEQ STREAM WORK B30 & 27.2 \\
%CWORK CABSEQ STREAM WORK & 47.3 \\
%CABSEQ STREAM WORK NP & 32.1 \\
%DIAGS5 RM NP & 32.0 \\ \hline \hline
%CABSEQ WORK STREAM & 49.5 \\
%CABSEQ WORK STREAM DIAGS5 & 55.2 \\
%CABSEQ WORK STREAM DIAGS9 & 62 \\
%CABSEQ WORK STREAM DIAGS17 & 145 \\
RM NP STREAM CABSEQ WORK & 27.5 \\
RM NP STREAM CABSEQ WORK B30 & 26.7 \\
DIAGS5 RM NP STREAM CABSEQ WORK & 25.0 \\
DIAGS9 RM NP STREAM CABSEQ WORK & 25.4 \\
DIAGS17 RM NP STREAM CABSEQ WORK & 33.3 \\
DIAGS5 RM NP STREAM CABSEQ WORK B30 & 24.0 \\
DIAGS9 RM NP STREAM CABSEQ WORK B30 & 23.8 \\
DIAGS17 RM NP STREAM CABSEQ WORK B30 & 28.1 \\
\end{tabular}
\end{table}

\begin{table}
\centering
\caption{Divergence counteracts the effect of optimizations}
\label{tbl:daligner4}
\begin{tabular}{c|c|c}
type of reads & implementation & runtime (s) \\ \hline
uniform reads & STREAM & 35 \\
& STREAM WORK CWORK CABSEQ RM NP & 6 \\ \hline
3000 bp custom PacBio reads & STREAM & 20 \\
& STREAM WORK CWORK CABSEQ RM NP & 9 \\
\end{tabular}
\end{table}


Replacing 'if(\_\_clz(a1) == 32)\{abpos -= max;\}' with inline PTX gave a 4\% speedup.
Other inline PTX implementations did not give a consistent speedup.


\todo{combine this table with Darwin?}
\begin{table}
\centering
\caption{Runtimes for different Daligner parameters, one CPU thread, run on ce-cuda, 10x E.coli, denovo. Sensitivity and specificity are measured for A vs A, using a score threshold of 200.}
\label{tbl:daligner5}
\begin{tabular}{c|cccc}
parameters & runtime A vs A (s) & runtime A vs B (s) & sensitivity & specificity \\ \hline
default & 32.5 & 61.0 & 99.78 & 86.7 \\
h45  & 31.5 & 57.6 & 99.58 & 86.8 \\
k15  & 31.6 & 56.9 & 98.9 & 87.1 \\
k16  & 29.4 & 55.2 & 96.9 & 87.5 \\
\end{tabular}
\end{table}

\newpage

\subsection{Profiling}
Nvprof \cite{nvprof} was used to profile the experiments.

% cat 4 reads
For 3000 bp reads from a custom PacBio generator:
\begin{itemize}
\item CWORK reduces the L2 cache misses by 40\%, increases the global load throughput by 35\%, and the number of global load transactions by 12\%. \vspace{-10pt}
\item CABSEQ reduces the number of global load transactions by 15\%, and increases the global load efficiency from 11.8\% to 14.2\%.
\end{itemize}

% cat 1 reads
For 10000 bp uniform reads:
\begin{itemize}
\item DIAGS5 reduces global memory traffic by 25-30\%, and increases the number of executed instructions by 35-40\%. \vspace{-10pt}
\item B30 reduced global memory traffic by 28\%. \vspace{-10pt}
\item ABSEQ reduced the number of global load transactions by 80\%, increased the efficiency of global loads from 41\% to 91\%, reduced the number of L2 cache reads by 60\%, and L1 global loads by 80\%.
\end{itemize}

For uniform reads, the optimizations can reach global read and store efficiencies of 92\% and 96\% respectively, for fixed length simulated PacBio reads, this effect diminishes greatly.
The result of the divergence can also be observed in the runtimes in Table \ref{tbl:daligner2}


\section{Darwin}
\subsection{Runtimes}
Darwin has the same run configuration, A B C, where A is the number of CPU threads, B the number of GPU blocks, and C the number of threads in a block.
Table \ref{tbl:darwin1} shows the runtimes for CPU baseline with 8 threads, and incremental GPU configurations, for a small dataset.
Tables \ref{tbl:darwin2} and \ref{tbl:darwin3} show similar results for a larger, 50MB dataset.
Note that the speedup for TACC is measured against 64 CPU threads, and for ce-cuda against 8 CPU threads.
The configuration 'GPU GASAL CPBASES' has the same output as the CPU baseline, when using NOSCORE, the output differs slightly.
All configurations use STREAM if applicable.
Table \ref{tbl:darwin8} shows that the speedup holds for scoring schemes that favour gaps, and for schemes that favour mismatches/substitutions.
The scoring scheme is encoded as follows: (match, mismatch, gap\_open, gap\_extend).
The third scoring scheme is based on the default one from EMBOSS Water tool \cite{emboss}.

\begin{table}
\centering
\caption{Runtimes for different Darwin implementations, with 3000 bp custom PacBio reads, run with 8 32 64 on TACC, A vs B.}
\label{tbl:darwin0}
\begin{tabular}{c c c}
implementation & runtime (s) & speedup vs CPU 64 \\ \hline
CPU 64 & 25m47 & -\\
GASAL CMAT CPBASES & 93.7 & 16.5 \\
\end{tabular}
\end{table}


\begin{table}
\centering
\caption{Runtimes and speedup for different implementations, run on a 2x E.coli dataset with 8 16 32 on TACC, A vs A.}
\label{tbl:darwin1}
\begin{tabular}{c c c c c}
implementation & runtime (s) & speedup \\ \hline
baseline & 82 & -\\
BATCH & 87 & 0.94 \\
GPU & 279 & 0.29 \\
GPU CBASES & 269 & 0.30 \\
GPU CMAT & 83 & 0.99 \\
GPU CMAT CBASES & 82 & 1.00 \\
GASAL & 22 & 3.7 \\
GASAL CMAT & 10 & 8.2 \\
GASAL CMAT CPBASES & 11 & 7.5 \\
\end{tabular}
\end{table}

\begin{table}
\centering
\caption{Runtimes and speedup for different implementations, N = 800, run on the 50MB dataset with 8 32 64 on TACC, A vs A.}
\label{tbl:darwin2}
\begin{tabular}{c c c c c}
implementation & runtime & speedup vs CPU 64 \\ \hline
CPU 8 threads & 64m35 & - \\
CPU 32 threads & 29m54 & - \\
CPU 64 threads & 27m56 & - \\
BATCH 32 8 64 & 37m17 & 0.75 \\
GPU CMAT & 14m20 & 1.95 \\
GPU CMAT CBASES & 11m28 & 2.44 \\
GASAL & 11m40 & 2.39 \\
GASAL CMAT & 1m17 & 21.8 \\
GASAL CMAT CPBASES & 1m9 & 23.0 \\
GASAL CMAT CPBASES NOSCORE & 58s0 & 28.9 \\
\end{tabular}
\end{table}

\begin{table}
\centering
\caption{Runtimes and speedup for different implementations, N = 800, run on the 50MB dataset with 8 32 64 on ce-cuda, A vs A.}
\label{tbl:darwin3}
\begin{tabular}{c c c c c}
implementation & runtime & speedup vs CPU 8 \\ \hline
CPU 8 threads & 138m21 & - \\
%BATCH 32 8 64 &  &  \\
%GPU CMAT &  &  \\
GPU CMAT CBASES & 11m26 & 12.1 \\
GASAL & 11m46 & 11.8 \\
GASAL CMAT & 1m19 & 105 \\
GASAL CMAT CPBASES & 1m16 & 109 \\
GASAL CMAT CPBASES NOSCORE & 56s1 & 148 \\
\end{tabular}
\end{table}

\begin{table}
\centering
\caption{Runtimes and speedup for different scoring schemes, N = 800, run on the 50MB dataset with 8 32 64 on TACC, A vs A.}
\label{tbl:darwin8}
\begin{tabular}{c||c|c|c}
& (2, -1, -2, -2) & (1, -3, -1, -1) & (5, -4, -10, -1) \\ \hline
CPU 64 & 31m15 & 21m28 & 31m27 \\ \hline
GASAL CMAT CPBASES & 76.0s & 59.3s & 78.0s \\
speedup vs CPU 64 & 24.7 & 21.7 & 24.2 \\ \hline
GASAL CMAT CPBASES NOSCORE & 63.5s & 48.5s & 64.5s \\
speedup vs CPU 64 & 29.5 & 26.6 & 29.3 \\
\end{tabular}
\end{table}




%TODO add final profiling, identify bottleneck to make recommendations in Future Work




\begin{table}
\centering
\caption{Runtimes for different Darwin parameters, 1 64 64, 10x E.coli, denovo, A vs A. Sensitivity and specificity are measured using thresholds of 600 and 990 for score and length respectively.}
\label{tbl:darwin4}
\begin{tabular}{c|cccc}
number of seeds (N) & runtime TACC (s) & runtime ce-cuda (s) & sensitivity & specificity \\ \hline
1400 & 105.6 & 62.8 & 99.33 & 85.0 \\
1100 & 94.2 & 56.1 & 98.9 & 85.4 \\
800 & 89.4 & 48.5 & 97.8 & 86.2 \\
\end{tabular}
\end{table}



\begin{table}
\centering
\caption{Runtimes for different Darwin run configurations, 50MB dataset, denovo, A vs A.}
\label{tbl:darwin6}
\begin{tabular}{c|cc}
Run configuration & runtime TACC (s) \\ \hline
 8 32 64 & 1m9 \\
16 16 64 & 1m32 \\
16  8 64 & 2m6 \\
 4 32 64 & 1m19 \\
 4 64 64 & 1m17 \\
 1 64 64 & 2m25 \\
1 128 64 & 2m14 \\
1 256 64 & 2m28 \\
 32 8 64 & 3m8 \\
\end{tabular}
\end{table}

%TODO add ce-cuda runtimes as well?




\newpage



\subsection{Profiling}
%global load efficiency and stuff

\begin{table}
\centering
\caption{Profile data for different optimizations, run on the 50MB dataset, with 1 64 64 threads.}
\label{tbl:darwin_prof}
\begin{tabular}{c c c}
optimizations & global ld efficiency \\ \hline
baseline & 12\% \\
GLOBAL & 49\% \\
GLOBAL CPBASES & 73\% \\
CPBASES & 45\% \\
\end{tabular}
\end{table}

Coalescing of the GLOBAL array and coalescing of the packed bases did improve the global load efficiency, as shown in Table \ref{tbl:darwin_prof}.

The alignment kernel consists of two distinct phases: alignment and traceback.
The kernel was split into two kernels, and their runtimes measured, the alignment part takes 96\% of the time.
This makes optimizing the traceback part not efficient.

\begin{table}
\centering
\caption{CPBASES saves some time in alignment, but adds preperation time.}
\label{tbl:CPBASES}
\begin{tabular}{c c c c}
& preparation (s) & packing (ms) & aligning (s) \\ \hline
base & 4.2 & 54 & 41.1 \\
CPBASES & 6.0 & 96 & 38.9 \\
\end{tabular}
\end{table}

The optimization CPBASES coalesces the packed GASAL bases.
This requires a more complex preparation on the GPU size, which interleaves the unpacked bases.
For the 50MB dataset, ran with 1 256 64, the timing results are listed in Table \ref{tbl:CPBASES}.
The preparation and packing take more time, but this is compensated by a faster alignment phase.



Using inline PTX did not give consistent speedup for either implementation.
This should not be a large surprise, modern compilers are quite good at creating efficient low-level code.

\begin{table}
\centering
\label{tbl:darwin7}
\caption{Percentage spent in GACT, 130MB, A vs A, N = 800, w = 4.}
\begin{tabular}{c|c c c}
run configuration & total runtime (s) & GACT time (s) & percentage GACT \\ \hline
8 32 64 & 157 & 136 & 87 \\
1 128 64 & 378 & 328 & 87 \\
\end{tabular}
\end{table}

%For a 10x de novo 15\% error dataset that took 62.8 seconds, xx\% was taken by GACT.
As shown in Table \ref{tbl:darwin7}, during runs on the 130MB dataset, the majority of the time was spent on GACT.
This shows that despite large speedups, the bottleneck is still extending the seeds using GACT.
For the 1 128 64 run, 55\% of the GACT time is spent on the CPU, preparing the new GPU arguments, and handling finished alignments.
In the 8 32 64 run, this percentage drops to 15\%.

A common performance metric for Smith-Waterman based solutions is CUPS, which stands for Cell Updates Per Second, where a cell is a score element in the matrix.
For 10x de novo A vs A, the Darwin GPU implementation peaked at 11.1 GCUPS.
It did up to 13.3 GCUPS during the 130MB dataset, A vs A, and 13.5 for A vs B.


\section{Sensitivity and specificity}
The algorithms should ultimately be used to find overlaps, the sensitivity and specificity indicate the quality of the report output.
Sensitivity and specificity are defined as:
$$\text{Sensivitity} = \frac{TP}{TP + FN}$$
$$\text{Specificity} = \frac{TP}{TP + FP}$$
where TP, FN and FP are the number of true positives, false negatives and false positives respectively.
Sensitivity indicates how many of the overlaps that the aligner was supposed to find, were actually found.
Specificity indicates how many of the reported overlaps were actually real overlaps.
To boost sensitivity, if an overlap between A and B was found (de novo), the overlap between B and A is also assumed to be found.

PBSIM \cite{PBSIM} is used to generate synthetic PacBio datasets.
The only parameters that is not default is the accuracy, which is set to 0.85\%, to mimic the 15\% error rate of PacBio.
The distribution is also changed: 1.5\% substitution, 9.0\% insertion, 4.5\% deletion.
The reads are generated from an E.coli reference.
\todo{Ecoli ref was sent by Yatish, mention?}
For reference-based alignment (mapping), a read that is aligned to the reference within 50 bases of the original location is a true positive.
For de novo based alignment, a true positive must have an overlap of 1000 bases according to PBSIM, and be found by the aligner.
The run configuration of Darwin consists of three numbers, the first denotes the number of CPU threads, the second the number of GPU blocks that each CPU thread launches, the third the number of GPU threads in each of those GPU blocks.
Since Daligner is faster of CPU than on GPU, the CPU version is used, and the number indicates the number of CPU threads.
For Darwin, h (default:21) indicates the number of non-overlapping bases that the Kmers must have to constitute a seed, n (1400) is the number of seeds that are considered for that readpair, and w (4) is the size of the window for the minimizers.
The overlap length threshold is set to 990 during de novo alignment.
For Daligner, h (35) means the same, l (1000) is the minimum length that an overlap must have to be reported.
Both aligners can use a minimum score threshold to further filter the found overlaps.
This will be denoted by s.
The scoring scheme for Darwin is: match = 1, mismatch = -1, gap\_open = -1, gap\_extend = -1, unless noted otherwise.
Daligner does not actually calculate the score, instead the score is defined here as the length of the overlap in a, minus the reported number of differences.

Darwin is not designed to compare two different files against each other, but a particular file against itself.
This will be denoted by 'A vs A', if two different files are compared against each other, this is 'A vs B'.

%%% ref
% daligner

\figC{width=.8\textwidth ,clip,trim=100 240 100 240}{daligner_sens_spec_l_ref.pdf}{Sensitivity vs specificity tradeoff depends on the setting of l.}{fig:daligner1}

% darwin

\figC{width=.8\textwidth ,clip,trim=100 240 100 240}{darwin_N_runtime_ref.pdf}{Sensitivity and runtime depend on number of seeds (N), w = 1.}{fig:darwin1}

\figC{width=.8\textwidth ,clip,trim=100 240 100 240}{darwin_sens_S_ref.pdf}{Sensitivity as a function of score threshold (s) and number of seeds (n), the runtimes are shown in Table \ref{tbl:darwin3}, w = 1.}{fig:darwin2}

\figC{width=.8\textwidth ,clip,trim=100 240 100 240}{darwin_spec_S_ref.pdf}{Specificity as a function of score threshold (s) and number of seeds (n), the runtimes are shown in Table \ref{tbl:darwin3}, w = 1.}{fig:darwin3}

%%% de novo
% daligner

\figC{width=.8\textwidth ,clip,trim=100 240 100 240}{daligner_sens_S_denovo.pdf}{Sensitivity for different Daligner options, their runtimes are listed in Table \ref{tbl:daligner3}}{fig:daligner2}

\figC{width=.8\textwidth ,clip,trim=100 240 100 240}{daligner_spec_S_denovo.pdf}{Specificity for different Daligner options, their runtimes are listed in Table \ref{tbl:daligner3}}{fig:daligner3}

% darwin

\figC{width=.8\textwidth ,clip,trim=100 240 100 240}{darwin_sens_runtime_w_denovo.pdf}{Runtime and sensitivity for different window sizes (w).}{fig:darwin4}

\figC{width=.8\textwidth ,clip,trim=100 240 100 240}{darwin_sens_S_denovo.pdf}{Sensitivity as a function of score threshold (s) and number of seeds (n), w = 4.}{fig:darwin5}

\figC{width=.8\textwidth ,clip,trim=100 240 100 240}{darwin_spec_S_denovo.pdf}{Specificity as a function of score threshold (s) and number of seeds (n), w = 4.}{fig:darwin6}


Figure \ref{fig:daligner1} shows the clear tradeoff between sensitivity and specificity for reference-based alignment.
The exact sensitivities for l200 and l100 are 99.97\% and 100.00\% respectively.
PBSIM generates reads with a minimum size of 100, this is why lowering the default minimum overlap length of 1000bp increases the sensitivity in reference-based alignment.
If the input data is longer than that, or not aligning those shorter reads is acceptable, the setting for l can be higher.
Figures \ref{fig:darwin2} and \ref{fig:darwin3} show a similar tradeoff between sensitivity and specificity for Darwin.
While the default value for the number of seeds is 1400, this number can be lowered to 400 without noticable impact on the sensitivity.
When going lower than 400, the sensitivity is reduced, but the runtime is also lower.
The runtime and number of seeds have an affine relation, so trading runtime for sensitivity is easy.
The specificity is higher when the number of seeds is low, it makes sense that producing less GACTcalls by reducing the number of seeds in the seedposition table reduces the number of false positives.
However, the N = 1400 and 800 options have a higher specificity when also using a high score threshold.
A tradeoff between sensitivity and specificity can also be observed from the figures.

Figures \ref{fig:daligner2} and \ref{fig:daligner3} show the same tradeoff as the others.
The score thresholds below 200 give the same result, that is why part of the x-axis is missing.
The output reacts quite strongly when changing the score threshold from 220 to 260.
Larger Kmers result in lower sensitivity, and higher specificity, as can be expected.


Figure \ref{fig:darwin4} combines the sensitivity and runtime, for different windows sizes.
The sensitivity shows a peak when window size is four, so this is the default size for the de novo experiments.
For the last two experiments, the sensitivity remains constant between score thresholds 0 and 600, so part of the x-axis is missing again.
The exact threshold where the sensitivity starts dropping will depend on the scoring scheme.
It makes sense that there is such a threshold, since the theoretical overlaps have a minimum length of 1000 bp, which should produce a certain score given the scoring scheme and the distribution of errors.



\end{document}










