
http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation:
PARTLY CITATION:
A warp consists of 32 parallel threads. Individual threads start together at the same program address, but they have their own instruction address counter and are free to branch and execute independently. A warp executes one common instruction at a time. If threads diverge via a data-dependent conditional branch, the warp serially executes each branch path taken, disabling threads that are not on that path. When all paths complete, the threads converge back to the same execution path. Different warps execute independently regardsless of whether they are executing common or disjoint code paths.

Threads can be active or inactive. Threads can be inactive because they finish earlier than other threads within the warp, or because they are on a different branch than the branch currently executed by the warp, or because they are the last threads of a block whose number of threads is not a multiple of the warp size.
---------- end citation
So if multiple diagonals in one alignment are calculated simultaniously, this will take as long as the time needed to find the longest snake in that d-wave. Trimming and pruning might be done in parallel.

From: CUDA programming A Developer's Guide to Parallel Computing with GPUs, Shane Cook, page 93
The scheduler is half-warp based, so you can execute two sides of a branch simultaniously, if 16 threads take the branch, and the other 16 don't take it.

----
Higher level parallelism:
Aligning reads A & B, and C & D is independent, so in theory, each thread in the warp could align its own two reads. But then the same problem occurs: aligning is very branching/while loopy. Thread 2 aligning C & D has to wait to find a new snake until thread 1 aligning A & B has completed its snake.
----
Ignoring the 32 threads per warp:
Each warp can execute one instruction at the time, since aligning is not very predictable, branches will occur, decreasing the performance. We can give each warp their own two reads, and not worry about the 32 threads.
----
The GTX 750 Ti has 640 cores = 20 SMs, if we have one alignment per SM, we need to store 40 reads in global memory. Or 2 reads per SM memory.
----
24-5-2017
Iterations of the while loop (test3) are not parallelizable, 'besta' and 'lasta' could be changed every iteration. This loop has a comment 'compute successive waves until no furthest reaching points remain'.
Iterations of the for loop (test4) are could be difficult to parallelize, since each iteration reads from and writes to arrays V, T, M, HA and HB. However, they only write to 'k', which is strictly increasing. Reading is done at the start of the iteration from either 'k' or 'k+1'.
Variables n,t,ua and ub are used to store information between successive iterations, but they are used to store the old T[k],M[k],HA[k] and HB[k], this information is also available at the start of the iteration.
The pointer to read 'a' is incremented each for-iteration, to mimic moving one row down in the edit graph.
29-5-2017
bseq is the same for each iteration of the test4 loop. a is the same as well, but incremented. Send a and bseq, and let each thread increment a by threadIdx.x.
1-6-2017
am, ac and ap form a moving window. Where V[k+1] gets inserted from the right each iteration. To parallalize this, you can read from V[k-1] to V[k+1] at once. However, c is stored in V[k] at the end of the iteration, and read by the next iteration via V[k-1]. c is only available after finishing the snake, this could mean the iterations need to be pipelined.
22-6-2017
To implement coarse grained parallelism, the big for loop in report_thread() needs to be split into parts. First, all calls to Local_Alignment() need to be recorded. After all the seed hits are processed (or 32 Local_Alignment() calls are recorded), the calls can actually be made. These calls might need to be load balanced first, because multiple CPU threads can filter. These filtered and balanced LAcalls can be dispatched to the GPU, possibly per 32. After all Local_Alignment() calls are made, the post-processing can occur. Like Diagonal_Scan() and Handle_Redundancies().

A problem is seed hits like diag=100 and diag=200. Both seed hits are recorded. In a sequential algorithm, seed hit 100 is calculated, this overlap might be longer than 100 bases, so seed hit 200 does not have to be calculated. However, in a parallel algorithm, both seed hits are extended at the same time. In theory, when seed hit 100 hits the 200th position, they can be merged into one overlap already. In practice, they might have to be combined by Handle_Redundancies().

To record the LA() calls, the arguments (Alignment *align, Work_Data *work, Align_Spec *MR_spec, int diag, int diag, int antidiag, -1, -1) are needed. The last five arguments are ints, and only two ints have to be recorded. The *MR_spec is the same for all threads and thus all alignments. Each thread has a Work_Data object, but this is empty at the time of the LAcall, so this need not be stored.

Each thread in filter.c has Overlap ovla and ovlb and an Alignment align. align->path = apath, where apath = ovla->path.

Match_Filter() allocates NTHREADS*3*w*sizeof(int) memory for diagonal buckets, where w is observed from 30 to 1088. These buckets store the score to filter, but also the lasta to see if current seed hits is probably already in an Overlap. If LAcalls are processed in parallel, this lasta function becomes unnecessary. The score array is not needed after the LAcalls have been stored. The lastp records the last position of a seed hits, to prevent overlapping seed hits from causing too high scores, this array is also not needed after the LAcalls.

23-6-2017
There is a seperate CPU function that only dispatches LAcalls, the postprocessing is done in another function called post_thread. This thread needs to process each LAcall, but some calls have dependencies when they belong to the same read pair. To keep track of which read pair an LAcall belongs to, we can store the readIDs, or we can store n, where n is the nth seed hit for a certain read pair. Because the second way uses less memory, we use that one.

24-6-2017
There is a problem with calling LA on each seed hit: the extensions of seed hits can overlap eachover. This problem is described on 22-6-2017, line 34. Three options are possible:
- do nothing, let Handle_Redundancies handle these overlaps
- let each LAcall know where the next LAcall starts, so that it can stop in time, this might prevent reverse_wave() to be needed for the second LAcall
- create a GPU thread for each read pair, instead of each LAcall.

27-6-2017
It is possible to combine a few LAcall arguments. Each pair has a unique Align object, this means that a pair with multiple LAcalls has redundant information wrt to the Align object. A new version of LAcall, where one LAcall is made for each read pair, and this object contains a dynamic array containing the seed hit positions, is a possibility to reduce space.

28-6-2017
Tesla K40c has 15 SMX, with each 4 warp schedulers. The warp scheduler selects an eligible warp and issues 1 or 2 instructions. One warp has 32 parallel threads. Each half-warp has to perform the same instruction. Shuffle allows threads within a warp to share data. Grid -> threadblock -> thread. User creates threadblocks, each threadblock is assigned to an SMX. The SMX divides the threadblock into warps. All threads of a warp belong to the same threadblock. Start with 32 warps/SMX, adjust from there. Active threadblocks use resources on an SMX, they are release as the threadblock is finished.

Thread resources:       local memory (as slow as global memory!!!)
Thread block resources: shared memory (very fast), barriers
Grid resources:         constant memory, texture/surface bindings
Global resources:       global memory

Possible parallel implementations:
- read pair = warp
	- LAcall = thread
	- need 32 LAcalls to occupy warp, but median seed hits per read pair is 7!, much divergence
	- more work is done, no if(apos<lasta[diag]) filter
- read pair = thread
	- 32 read pairs per warp
	- less work, because filter if(apos,lasta[diag]) can be used
	- some read pairs are done quickly, much divergence
- read pair = dynamic
	- first: read pair = thread
	- as read pair is done, the thread would becomes idle for the rest of the duration of the warp, but assign a new read pair to the thread. Thread ends when no read pair is available. Make sure that those threads belong to the same threadblock, so they can use shared memory to indicate which read pair they are doing. This indicates we need big threadblocks, as oppose to many.

	if(new read pair needed)
		if(getNewReadPair() == fail)
			exit thread
	for(all seed hits of read pair)
		while(apos<lasta[diag])
			skip seed hit
		_syncthreads()
		LA()

29-6-2017
To let the warp start the LAcall, all threads should have a seed hit ready. Threads should only exit when there is no read pair left, this should only happen when the threadblock is almost fully computed.
Memory accesses are per warp, memory is accessed in discrete chunks. This will be slow. Some threads advance quicker through the read (longer/more snakes). Different threads also have different active diagonals and d-wave widths. These non-coalesced memory accesses will cause multiple memory transactions. One transaction is a 128B line (when caching) or 1,2,4 32B segments (when non-caching). Maybe SCORE/TABLE can fit in constant cache. We need to saturate the memory bus (latency*bandwidth bytes in flight). GK110 needs ~100 lines/SMX in flight, each line is 128B. To increase concurrent accesses: increase occupancy (more warps, adjust threadblock dimensions on register and smem or reduce register count), or increase number of elements read per thread. Smem: 32 banks, each 8B wide. Choose 4 or 8B access. Each bank holds 32*8B=256B. Bank conflict when multiple threads access different 8B words in the same bank. Smem size: up to 48KB. Each threadblock needs some smem to indicate which reads are done, each threadblock needs enough smem to be active. 48KB/N active threadblocks possible where N is amount of smem one threadblock takes.
Watch out for register spilling.
Tail effect limits max performance when threadblocks are too big or too few. 1000+ threadblocks are recommended.
Don't just choose big threadblocks. Look at the register and smem sizes to maximize the number of warps/SMX.

30-6-2017
Reduce registers from https://devtalk.nvidia.com/default/topic/597219/once-again-for-registry-spills-performance-and-nvcc-magic/
- recalculate easy values instead of storing them
- use volatile to force recalculation
- compress values into short2 or char4, indices to shared arrays and small counters are good
- save expensive calculations to smem and perform broadcast read when value is needed (probably not useful for us)
- give C preprocessor and compiler opportunity to expand/collapse/fold constants and code
- avoid arrays with local extent
split larger algorithms into multiple smaller kernels, more overhead to gmem between kernels, but smaller kernels have less registers

10-7-2017
It is possible to use m/realloc() on device, but the resulting pointer cannot be accessed by cudaMemcpy(). A solution is to allocate a very big buffer for the output-to-file data, or let the device use realloc(), copy the device pointer to host, use cudaMalloc() to allocate enough space, send the device pointer to device via another kernel, copy the data from the device pointer to the cudaMalloced pointer, and let the host use cudaMemcpy on the cudaMalloced pointer. However, this seems like an inefficient process.

19-7-2017
Previous solution to copy data back to CPU needs to preserve data across the kernels, this could be difficult.
Bug fix: printing 64 bits for ahits when ahits is int caused problems
Bug: LAshow uses the readIDs for the db, which are the original reads, the readIDs that are written to file are the mapped reads. Solution: give mapping to GPU, the GPU will unmap before writing to out->buf.

20-7-2017
Unmapping done, LAshow now has the same results for the GPU and original implementation. But the hexdump shows that the GPU has 4B trailing, and the pointer to ovl->path->trace is visible in the hexdump (not via LAshow) and cannot be altered easily. The GPU implementation writes 0s to those bytes.

21-7-2017
Trying the 6.8MB set, the out->buf is out of space. Increased size to 100KB, but since the final las files can be very large, this needs work to make it scalable. E.g. the host could keep spawning new instances of the GPU kernel until all the readpairs are finished.

22-7-2017
A dataset with 25000 lines (1.7MB) is fine for 8 GPU threads. The 6.8MB set accesses an illegal memory address.

23-7-2017
Changed d_realloc manual byte-by-byte copying to memcpy.
Multiplied oldsize argument by sizeof(Path) in d_realloc call for a/bmatch.
3.4MB set is ok, work on 6.8MB
The rpairs with more than (~300) ovls that cause a d_realloc of tbuf->trace are the only ones different from the original implementation. The only aread that has this is 798 for the 6.8MB set.
Multiplied oldsize argument by sizeof(short) in d_realloc call for tbuf->trace, does not seem to make a difference

24-7-2017
Removed a lot of 'sizeof(short)' for the d_realloc call for tbuf->trace. tbuf->top/max indicate the number of trace points.
3.4MB set is ok
6.8MB set is ok, despite numerous reallocs for tbuf->trace
6.8MB: T8 : 26m
       T16: 28m, wrong output due to could not allocate for Allocating paths
No complement yet.

25-7-2017
Increased heap size to 20MB (from 8MB), probably per grid or device.
CBT notation: x,y,z: x number of CPU pthreads, y number of blocks per CPU pthread, z number of GPU threads per block

26-7-2017
Created superrun.sh to run GPU daligner with different GPUTHREADS and HEAPSIZEs.
Memory leaks might be causing problems when computing the complement.

27-7-2017
Added extra free() and cudaFree() calls to reduce memory leakage.
LAmerge cannot or does not write all overlaps to final las file since the moment the complement is also computed.
Output is wrong for complement only runs.

28-7-2017
LAshow only reads N overlaps from the GPU implementation, while half of them should be C overlaps. The GPU only writes 0 as flags, indicating N reads, the flags are not passed to the ovla that is written. Fixed: d_MG_comp indicates if current DB is complement.
Output including complement is correct for 3.4MB set.

1-8-2017
Align_Spec->score now is preserved for the complement.
The asettings variable in daligner.c cannot be freed, it apparently already is.
Output including complement seems correct for 6.8MB set.

2-8-2017
Output including complement is correct for 6.8MB set.
Created x_scalingrun.sh, view_results.sh

4-8-2017
Cannot run with 512 threads per block: each thread uses 255 registers, and 512*255=130560>64k max regs/SMX.
cuda-memcheck.sh fails when using 2100 MB heapsize, 2000 MB is ok, happens with both 63 and 255 register/thread. Fixed, HEAPSIZE was 32 bit, which limits the max heapsize to 2.2GB.
run.sh works for 128 regs/thread and 512 threads, more than 128 fails, as expected, 64 regs/thread and 1024 threads busy
The complement was actually not compared, some values are wrong. h2.h1 is wrong, mainly n/c faults, and new- has a lot more overlaps.
h2.h1 is wrong for 3.4MB too
Let's compare the files before external sorting.
The written ovlb.flags did not have the correct value of d_MG_comp, this is why only the h2.h1 files were wrong, fixed now.

6-8-2017
Removed some unused parameters for _LAparams.
Started to run 130MB set, stopped after 2h and no "calling cuda_host_function" msg was printed.

8-8-2017
Multiple GPU_BLOCKS are used now, added BlockPar struct.
Works with 2 blocks and 2 threads for 3.4MB.
Works with 4 blocks and 8 threads for 6.8MB.

9-8-2017
Some threads don't have an LAcall when other do, this means that the uptime of those threads is very low. To prevent this, we need to make sure that each thread has an LAcall when the warp reaches that instruction.

11-8-2017
Fixed bug: i was used to keep track of which LAcall was used, but it was also used somewhere else, and it reset constantly.

12-8-2017
get_new_readpair() now returns -1 when no new readpair was found, instead of 0.
Dramatically changed order of pre/main/postprocessing, see 9-8-2017 draft.
Now works for 1.7MB, need to remove cripple to test for 3.4MB.

13-8-2017
Works for 6.8MB, singlethreaded
Trying with multiple threads, both files miss one overlap.
Changed atomicExch to atomicMax.
Still not every thread has an LAcall when calling Local_Alignment()

20-8-2017
Added atomic counter to check if enough threads have an LAcall, but still only one thread is doing LAcall after the first readpairs are done.

21-8-2017
Removed atomic counter.
Still both files miss one overlap, for 256 threads, but not for 64 threads, 3.4MB.
Reverted to 8-8-2017 commit.
3.4MB is ok, also for multiple threads, not checked for multiple blocks.
Possibly implement a round-robin scheme:
if(1/10 chance){
	LAcall()
}
to allow other threads some time to get an LAcall as well, see source [1].
Possibly implement thread-remapping, source [2].

22-8-2017
Fixed blockpar calculation again.
Removed bpar param from _LAparams, blockIdx.x can be used.
Ran with more blocks, very little speedup.
Used Nsight to profile, as expected low occupancy, but no very informative results.
Started to implement delayed iterations, see source [1], stopped after talk with Zaid.
Talked to Zaid: try implementing LAcall() only, instead of the whole Readpair assignment thing on GPU.

23-8-2017
Started with justLA branch

24-8-2017
justLA works for 1.7MB, singlethreaded.
Worked on read_generator again.
justLA does not work for 3.4MB, singlethreaded, lots of overlaps are missed. It seems just one LAcall is computed for many rpairs, especially the middle and last ones.

25-8-2017
Created read generator in c.
justLA works for 2.1MB, up to 4 threads, except LAshow shows a bb/be pos of over 1 billion, but the hexdump is correct. DB from 1 and 6_8MB show correct values.
justLA works for 0.4MB, up to 5 threads, 6 threads miss some overlaps: not enough memory, but tbuf->max is over a billion, was because tbuf->top was not initialized.
justLA works for 0.4MB, up to 32 threads, 64 results in illegal memory access, probably because there are not enough readpairs.
justLA works for 2.1MB, 128 threads, except LAshow still shows values over 1 billion.

27-8-2017:
amatch was too small, and not realloced dynamically, added error message

28-8-2017:
Created best/worst/perfect reads in read_generator.
Writing Path instead of Overlap from GPU to buf.

29-8-2017:
justLA works for 2.1MB, 128 threads.

30-8-2017:
justLA CPU scheduling had apos<lasta, instead of apos<=lasta.
justLA works for 6.8MB, 512 threads.

1-9-2017:
Created justLA_multiple_pairs
Worked on generating perfect reads to show that GPU can be faster in ideal situation

4-9-2017:
Used NSight to profile, computation part seems 2s for 10000 best reads.
Merged individual out->buf1/2 into one large buffer that holds GPU output for each thread, both buf1 and buf2.
Persistent work struct, preallocd on CPU, probably cannot be resized on device

5-9-2017:
Bug fix: tmp buffer for reading overlaps was not offset properly for h2.h1
Works for 100k worst reads, CBT: 32 16 32 and 16 32 32
Changed read_generator: synthetic starting point now only causes 3 LAcalls

6-9-2017:
Removed blockPar.
Works for 100k best reads, CBT: 4 36 256, 64 regs/thread
100% occupancy for CBT: 8 32 128, 3500MB heap, 32 regs/thread, still slower than CPU
justLA_multiple_pairs works for 2.1MB, 1 thread

7-9-2017:
Best reads: 1100 10000:
- CBT: 8 32 128 has 13% occ, but 12% gmem eff, is 3x slower, 128 regs/thread seems to be slightly faster than 32 regs.
- CBT: 2 128 512 has 97% occ, but 12% gmem eff, is 5x slower, 32 regs/thread
- CBT: 1 128 1024 has 97% occ, but 12% gmem eff, is 6x slower, 32 regs/thread

8-9-2017:
One rpair with 1100 bases each, one thread, 248 regs:
local load: 46, store: 57
shared load: 1, store: 1
global load: 8174, store: 3031
bseq[] reads: 1503
   a[] reads: 1499
 TABLE reads: 117
 SCORE reads: 38
  work reads: 11 (no reallocs needed)

11 - 15-9-2017:
Worked on thesis

18-9-2017:
Moved flexible debug printing to histogram to measure number of reads and writes to bseq, TABLE, work, work->vector and num Pebbles and num tracepoints.
Cachable: bseq, a, V, M, T, ..., Pebbles (wait until 5 Pebbles, then writeback to gmem), tracepoints (wait until 20 tracepoints, then writeback to gmem).
Coalescable: bseq, a, VectorEl from work->vector, Pebbles, tracepoints
Sizeof smem per thread: 48kB/(num threads).
Regs/thread: 128 -> max 512 threads/SMX
Threads/block: 128 -> max 4 blocks/SMX
48kB / 512 = 96 B
sizeof(Pebble) = 16 B -> don't cache Pebbles, coalescing possible
sizeof(VectorEl) = 32 B
sizeof(Path) = 32 B
sizeof(*) = 8 B
Create coalescable crosstalk by making every 88 reads the same, instead of 95
main for loop (test4) has mostly k as index for work->vector arrays, easily coalescable if they have the same reads
97.1% of work->vector  reads are in the main for loop (best 1100 10000)
93.8% of work->vector writes are in the main for loop (best 1100 10000)
Maybe unmapping or LAcall array can go in constant memory
Gmem transaction size: 32B (non-caching) or 128B (caching)
TABLE read indices are quite random and well distributed, but median is 27k, max is 32k -> only store top half of TABLE in constant memory. This is because there are more ones than zeroes in 'b'.

19-9-2017:
Started coalescing bseq and a, not tested yet
Cmem cache size is 8kB, but it is probably slower than gmem when reading different addresses.
Started coalescing MTHAB arrays, not tested yet

21-9-2017:
99.6% of work->vector  reads are in the main for loop (6.8MB)
99.0% of work->vector writes are in the main for loop (6.8MB)
Redone profiling on /dev server, no other servers have the mcprof tools

26-9-2017:
Successfully made workflow to TACC

27-9-2017:
ABSEQ did not work, started to work on MTHAB
CBTr notation: r for number of regs per thread
Reverified baseline with 12% gmem efficiency on TACC, stored in coaleasce.txt
MTHAB seems to work for 2.1MB, gives ERROR MOVING IN FORWARD WAVE for larger sets and worksizes 16k 5k 15k, but gives 'misaligned address' when using 20k 5k 15k.
It looks like this error is caused by mthab[k].T = PATH_INT;, since gdb claims the error is in the line after it, no matter which order it is in.
'misaligned address' also occurs when using 17k, 22000, 20064, 20072, 20080, 20088 for work_vector_size, but not when using 18k, 17024, 20032, 20056, 20096

28-9-2017:
Forgot to implement MTHAB in reverse_wave, did now
Tested MTHAB gmem bandwidth and efficiency for option 1: put MTHAB next to eachother for each thread.
		best:	MTHAB	VMTHAB		worst: VMTHAB wrt to base and in %
LD Transactions: -11		- 0			- 0
ST Transactions: -15		-35			-35
LD Efficiency:	 +11		+ 0			+ 0
ST Efficiency:	 +15		+35			+53
ST Bandwidth:	 +29		+20			+34
LD Bandwidth:	 +34		+60			+100
Turning off cache is almost useless:
Transactions: -0.00%
Efficiency: +0.00%
Bandwidth: +0.4%
MTHAB option 2: interleave all M arrays, all T ararys, seperate

29-9-2017:
Talked to Zaid

2-10-2017:
Fixed ABSEQ coalescing, works for best reads and 2.1MB
ABSEQ coalescing almost works for 6.8MB, C=150k recommended, hangs at different readpairs for different CBT configs, very weird

3-10-2017:
Fixed bug where more than needed bytes were allocated for the bases
Did some profiling with ABSEQ coalescing.

4-10-2017:
More profiling with ABSEQ coalescing
Created z_compile.sh

5-10-2017:
L1 cache is opt-in, redone profiling
Started to coalesce whole work struct, including vector, cells and points.

6-10-2017:
Talked to Nauman about compressing. He has 4 bit per symbol, compressed 8 into 32-bit ints. Gave him 200x speedup. His uncompressed bit-array has no sequence delimiters, uses a table with offsets for that. Also uses padding if seq-length is not a multiple of 8, to make sure a 32-bit int has data for only 1 sequence.

9-10-2017:
Fixed bug that screwed up COALESCE_WORK, it seems to work now for 1 thread.
Multiple threads cause misaligned address error at bpath->trace = ... in Local_Alignment
Bug: bpath points to the begin of work->points, proposed solution: remove bpath from work->points, put into seperate array
Invalidated LAcall_thread() and post_thread() by removing bpath from LAcall struct (parallel.h).

10-10-2017:
At the end of reverse_wave(), abpath->trace is redirected to point at the first trace point.
Fixed major bug: NA[low] was not coalesced at a few points.
COALESCE_WORK now works for 300kb dataset.
COALESCE_WORK now works for 2.1MB dataset.

11-10-2017:
Created benchmark.py to benchmark for wallclock speed.
Changed time seed in generate.c to constant seed '1946162915'

12-10-2017:
More benchmarks
Plan: implement caching in smem, even though there is very little space
Plan: preallocate space for a lot of less-used variables, so that often used variables can be put in registers

13-10-2017:
Started caching often-used variables in smem.

23-10-2017:
Profiled some more with 200k reads no crosstalk
Made timeline for 200k reads no crosstalk, WORK and ABSEQ, 8 32 128 64r
Massive bug discovered: wrong number of diffs for 1100 length reads
Fixed: a line 'mored = diffs' was accidentally deleted, put it back now

25-10-2017:
Fixed smem for two variables (c and d)
Made extra macros for variables in smem, to replace #ifdef CACHE_VARS constructs
Variables in smem: c,d,y,m,ha,hb,b for CACHE_VARS option
Profiling shows that this CACHE_VARS option has no real influence on runtime or any of the profiled metrics/events.
Base version spills 1264 stores and 1228 loads for 64r, 624st 924ld for 128r
CACHE_VARS spills 1208 stores and 1212 loads for 64r, 684st 1156ld for 128r
WORK spills, non-volatile: 640st 912ld 128r, volatile: 616 844

26-10-2017:
Profiled CACHE_VARS:
				with WORK and ABSEQ	without WORK
read_misses		-25%				-6%
read_queries	-7%					-3%
gst_req_BW		-23%				-10%
gld_req_BW		-21%				-10%
Influence of COALESCE_WORK: hitrate L2 cache: 30% -> 70% and 45% -> 80%
Influence of WORK and ABSEQ: total L2 read queries: -70%

31-10-2017:
CACHE_DIAGS works with 32 threads, not more, need a new layout.
Initial tests indicate CACHE_DIAGS is slower than base.

6-11-2017:
Profiled stuff:
CACHE_DIAGS5 wrt to base causes most metrics to worsen a bit.
DIAGS5 WORK ABSEQ wrt WORK ABSEQ:
gst_req_BW: -14%
gls_red_BW: -10%
l2_s0_read_miss_ratio: +24% (27.7 -> 34.3)
l2_s0_reads: +2%
DIAGS9 WORK ABSEQ wrt WORK ABSEQ:
gst_req_BW: -40%
gls_red_BW: -40%
l2_s0_read_miss_ratio: -67% (27.7 -> 9.1)
l2_s0_reads: +3%

13-11-2017:
New idea: coalesce Pebbles, since they are an array of structs, should be a struct of arrays (or just seperate arrays), also compress them:
Pebble compression: pb->ptr: 14b, pb->diag: 18b, pb->diag: 15b, pb->mark: 17b
pb->diag can be negative, so give it an offset of 2^17=131072
Pebble are now coalesced and compressed, speedup still around 10% vs base, so no apparent improvement by coalescing and compressing Pebbles
work->vector compression: HAB: 14b, NAB: 18b, M: 11b, V: 21b
NAB is 18b, but is stored in pb->mark, which is 17b, maybe give an extra bit to HAB.

16-11-2017:
HAB and V have a min value of -1, so they have an offset of +1 to make it unsigned
gridDim.x and blockDim.x are probably unsigned, multiplying with an idx of -1 will cause trouble.

20-11-2017:
Switched V and M in compression, V is now upper 21b, M lower 11b.
Removed -1 offset for compressed V.
Max value of V is not INT32_MAX, but (1<<20)-1, when compressing.

22-11-2017:
Profiling of compressed and coalesced Pebbles and compressed work->vector (41):
41 wrt 22:
gld_eff: 72.14% -> 81.11% = +12%
gst_eff: 66.71% -> 73.07% = +10%
gst_req_BW: -3%
gld_req_BW: +20%
l2_reads: -13%
l2_writes: -15%
32b_instructions: +30%
l1_lcl_ld_hit: -13%
l1_lcl_ld_miss: -91%
l2_s0_read_miss: -57%
l2_s0_tot_read: -13%

Changed clock() timestamps to gettimeofday() timestamps in cuda_host.cu
Created kernel_time.txt

27-11-2017:
Implemented CPU streaming cuda functions, initial tests show 10-30% speedup wrt to 41.

28-11-2017:
Benchmarked streaming cuda functions, is 5-25% faster than 41.
Compressed and coalesced Pebbles are slower when using non crosstalk worst reads.

29-11-2017:
Timestamps show that cuda functions overlap.
Pinned memory is not faster.
Created branch 60 to compress/cache ABSEQ.

30-11-2017:
Measured number of times a base is actually read to predict whether caching will be beneficial, number is not too high, estimated 1-5x.
Measured difference between first read a[y] in wave vs lowest address a[y] in wave (for_wave). When the first read a[y] is higher, it cannot be used as an anchor to start caching bases from.
Also measured the difference with the highest address a[y] in for_wave.
A width of 45 cached bases is probably the best range.

1-12-2017:
Created view_results.py to format measurings of distance between first ab[y] read, and other ab[y] reads to determine CACHE_ABSEQ pattern.
To cache 95% of ABSEQ bases, 40 bases are needed for aseq and bseq. 32 for bseq probably hits 90%.
Zaid: remove stuff to see where the bottleneck is, ignore accuracy for now

4-12-2017:
Created breaking.txt and git branch 'breaking'.

7-12-2017:
Benchmark for NOPEBBLES: 16.3, 25.5, 3.3, 14.0, 9.1, 1.3, +0.8, 4.1, 10.0, 18.7, avg 10% faster
Created option FAKE_ABSEQ

12-12-2017:
FAKE_ABSEQ is 1% slower than base for best reads with snakelength 8.
WAVE_LEN=20 gives 0-10% speedup for GPU, but 33% speedup for 130MB on CPU.
WAVE_LEN=35 gives 7% slowdown for cat 4 reads on GPU, and 10% slowdown for 130MB on CPU.

13-12-2017:
Created branch 'min2', to reduce min(ac,ap,am) to min(a1,a2).

14-12-2017:
Worksizes 10000, 10000, 1000 works for ./generate.sh 4 3000 10000 5000 16
Measured impact of register spilling
./generate.sh 4 3000 100000 5000 [1|2] is faster than CPU implementation!!! (ce-cuda only)
Created 'benchmark.txt'

18-12-2017:
Created 'bittricks.txt', this explains bases can be compressed into 32 bit words, which can be shifted and XORed to find the snakelength. This removes the need for a while loop. Careful with abclip though: abpos have to be compared to ablen to detect end-of-reads. 99.9% of snakes from 6.8MB are shorter than 10, so 32 bit is enough. A branch should be created for snakes longer than that. The snakelength can be added to abpos, and to the number of matches 'm'. For 'b', a note is made in bittricks.txt.
Created branch '80', for implementation above.

3-1-2018:
bitwise comparison of bases works for all reads

6-1-2018:
Replaced 'int max = t2; if(t4 > t2){max = t4;}' with intrinsic min/max functions, no speedup
Replaced 'if(__clz(a1) == 32){abpos -= max;}' with 'abpos -= max * (__clz(a1)==32)', no speedup
Replaced 'abpos -= max * (__clz(a1)==32)' with inline ptx, 4% speedup
Replaced intrinsic min/max function with bitfiddle, no speedup

14-1-2018:
Reordered calculation of min/max/t2/t4 to decrease number of instructions, 4% speedup.

15-1-2018:
A big problem is that the GPU is little-endian, this means that the bytes in an int are reversed, however, the bases inside the byte are not. This is fixed in the function revint(), but it is probably slow. A better fix is to compress the bases differently.
Current compression: 1       2       3       4       5       6       7       8
					 1 2 3 4 5 6 7 8
Proposed:			 1       2       3       4       5       6       7       8
					 5 6 7 8 1 2 3 4
This compression is done in cuda_host:compress()

int = idx % 16
byte = 4 - idx % 4

TACC CPU and ce-cuda CPU have very different runtimes!!

16-1-2018:
Reimplemented CACHE_DIAGS from branch justLA, it had a big bug, which is fixed now.
Reordered clz usage, no speedup

17-1-2018:
Combining CACHE_DIAGS with COMPRESS_ABSEQ is (much) slower
Base: 214r, 82: 197r
83: 82+DIAGS5: 232r, possibly due to pointers to smem
82 cat 1 reads:
- uses about 55% of global ld/st BW vs 22, same ratio for l2 ld/st instructions
- L1 hitrate (48KB L1) is 99%, vs 70% with option 22
82 vs 22 has little diffs in cat 4 reads

22-1-2018:
Removed Pebbles again, but more thoroughly, still ~35% speedup.
Removed SCORE and TABLE usage, overwrite trimX variables each time now, no speedup.
Still a lot of memory dependencies.
Made abseq const and __restrict__, no speedup.

23-1-2018:
Made VMTHAB arrays __restrict__, no speedup.
Use inline PTX to find max of ac,ap,am (only in for wave), no speedup.

24-1-2018:
Inline PTX for loading m and b from gmem works seperately.
Inline PTX for loading m and b from gmem works together, but only with -O0 or -O3 -G

25-1-2018:
Added 'volatile' to make it work without -G
Inline PTX for loading m and b from gmem works together with ptx to find max of ac,am,ap in both for and rev wave, small slowdown (1-2%).
Changing 'am>ap' to 'am>=ap' introduces differences in the results.
Put SCORE and TABLE usage back, no difference in runtime, slight increase in register spillage
Made SCORE and TABLE const and restricted, no speedup.
./run.sh 8 128 32 is 5% faster than 8 32 128
Caching align->ablen in registers, 9-13% speedup.
Inline PTX for keep track of besta and besty, no speedup.
Small reorder for 'min>>=1;a1&=mask' in snakefinding, <1% speedup.

26-1-2018:
Previous nvprofs show that removing Pebbles results in 10% less global loads, and 40% less global stores, this is mainly due to not reading/writing to HABNAB arrays and Pebble *cells array.

Compress VM: V: 21b, M: 11b, both are unsigned
When trimming the wavewidth, n is shifted right by 11 bits, otherwise V[] has to be shifted back each time.

Do not use predicated instructions to write to an output in inline ptx, rather predicate write then to a temporary register, and then write then unpredicated to the output.

29-1-2018:
Made lots of scripts suitable for TACC and ce-cuda, export TACC=1 in .profile_user on TACC.

30-1-2018:
Made CPU implementation without Pebbles, synced align.c manually on TACC/original and ce-cuda/bulk/original. Git commits: TACC:062818d, ce-cuda:8a78ff0
Both CPU implementations have the same output with and without Pebbles.
TACC GPU (STREAM CABSEQ WORK, no args) and CPU implementations have the same output with Pebbles.
Benched CPU implementations on TACC and ce-cuda.

31-1-2018:
Fixed misaligned addresses when NP, worksizes were no multiple of 8.
TACC GPU (NP, NP WORK, NP CABSEQ, NP WORK CABSEQ, NP WORK CABSEQ STREAM) and CPU implementations have the same output without Pebbles, commit da78a9cc.
CVM and NP are not compatible at this moment.
Also benched C16 and C32 on CPU, both TACC and ce-cuda.
Confirmed output CPU and GPU on TACC and ce-cuda.
Merged COALESCE_WORK code using __X.

Changed COMPRESS_VM into COMPRESS_WORK.
Plan: first implement compression, then caching via registers (vm_curr, etc.)
NA is used more often, so put it in MSB, HA will have an offset.
work->vector compression: HAB: 14b, NAB: 18b, M: 11b, V: 21b
NAB is 18b, but is stored in pb->mark, which is 17b when compressed, maybe give an extra bit to HAB.

1-2-2018:
Compressed VM, HNA, HNB with option COMPRESS_WORK in for wave, output ok 9e7ebe0

2-2-2018:
Changed work->vector pointer when COALESCE_WORK is active, all threads now point to the same address.
(no args, WORK) have good output in commit 6c115d4c, runtime 1m15

3-2-2018:
Fixed bug: GPU_THREADS ipv __X used, wrong result when not using WORK.

4-2-2018:
Benched CWORK
Started with CACHE_WORK (option CAWORK) in for wave, 92667911
(no args, CAWORK) same output end of for wave
(CWORK, CWORK CAWORK) same output end of for wave
(CWORK NP, CWORK CAWORK NP) same output end of for wave
(NP, CAWORK NP) same output end of for wave
Implemented CACHE_WORK in both waves, output ok for (no args, CWORK, CAWORK, CWORK CAWORK), aee92a5a

7-2-2018:
FALCON does not use tracepoints: https://github.com/PacificBiosciences/FALCON/wiki/Manual

8-2-2018:
Manual caching of NAB values while creating Pebbles, small slowdown, 3a49ebda
Started with COMPRESS_PEB and COALESCE_PEB.
Pebble compression: pb->ptr: 14b, pb->diag: 18b, pb->diag: 15b, pb->mark: 17b

12-2-2018:
Brought back the CPU only implementation, with split LAcall_thread() and post_thread(), 9cf41b03
Cat 4 reads typically have only 50% LA utilization (CPU compile option).
Real Pacbio reads still have 90%.

13-2-2018:
Measured for CACHE_ABSEQ (only with COMPRESS_ABSEQ) the number of ints that should be cached, commit 2e66cb30.
Talked to Zaid and Shanshan:
- measure all variants on ce-cuda
- measure for cat 1 reads, profile as well
- try to compress more, mainly diagonalinfo, even across diags
- benchmark real data

14-2-2018:
Bugfix: GPU threads were not shutdown properly, 7f52f651

15-2-2018:
Benched ce-cuda and real data
./convert 2 needs more than 1400 diags with CWORK CABSEQ STREAM, 2000 works
6.8MB needs more than 600 Pebbles per thread, 1000 seems to work
ce-cuda, ./convert 1 needs more than 1000 pntsize, 5000 works

19-2-2018:
Benched real data on TACC and ce-cuda CPU
Trying 6_8MB option 200 does not work with:
2k 1k 5k (diags, Pebbles, points_size)
2k 2k 10k
Works: 4k 4k 10k, needs 50k < GPU_OUTPUT_BUFFER < 100k, needs 600 < MATCH_SIZE < 800

21-2-2018:
Benched cat 1 reads on TACC and ce-cuda, CPU/GPU

22-2-2018:
Talked to Zaid
- measure TIME for kernel/copy/cpu/copy
- bench bigger cat 1 sets
- profile CACHE_DIAGS, should speedup because of less mem operations
- shrink B to 32b vector, could change output
- check out striped SmithWaterman
- radical approach: represent match/mismatch in bitmatrix, compute snakes from there

8-3-2018:
Maybe it is possible to represent matches from 1 diagonal in an int, and follow snakes there.

15-3-2018:
(B30 CABSEQ STREAM, B30 CWORK STREAM, B30 CABSEQ CWORK STREAM) output is ok

16-3-2018:
RM CWORK STREAM output ok
Bug: when a is realloced, a_int points to freed memory, only a problem when reallocing a or b

19-3-2018:
RM CWORK CABSEQ STREAM WORK NP output is ok
B30 NP STREAM CWORK WORK CABSEQ RM vs B30 NP STREAM output is ok

21-3-2018:
Implemented CACHE_DIAGS, RM forced, NP forced, only caches 64b B/T and 32b V
DIAGS5 RM NP vs RM NP output ok, ad4d27b2
DIAGS5 RM NP STREAM CABSEQ WORK vs RM NP output ok, ad4d27b2

22-3-2018:
Implemented B30 with CACHE_DIAGS
DIAGS5 RM NP B30 vs B30_gold_NP output ok, 9a127cb0
Checked clkspeeds TACC and ce-cuda with 'nvidia-smi -q -d CLOCK', same for both

23-3-2018:
Implemented ABSEQ with CABSEQ
CABSEQ ABSEQ vs 4_gold_P output ok, 07e4658c
CABSEQ ABSEQ NP vs 4_gold_NP output ok, 07e4658c

26-3-2018:
Another bugfix: sizeof out buffer was not calculated correctly in coalesce_compressed_reads, cdb2dbee

27-3-2018:
Talk with Zaid:
- try bigger datasets
- focus more on GPU based implementation, find matches/mismatches in GPU, calc snakes on CPU
- translate improvements in profiling to improvements in runtime

3-4-2018:
Finished implementing CABSEQ on original/CPU

5-4-2018:
Bugfix: the end of compressed reads would still have bases from previous reads, or the value '4' (0b100) which contaminated the previous (last) base, daca007
Trying bitmasks for quick ctz/clz calculation:
- 6_8MB: 77.4% clz is calculated in 2 bits
		 93.5% clz is calculated in 4 bits
		 97.8% clz is calculated in 6 bits
		 77.8% ctz is calculated in 2 bits
		 93.7% ctz is calculated in 4 bits

Removed some redundant ctz/clz calls
Using more bits for fast ctz/clz calculation is slower than the builtin on TACC.
Same runtime for TACC/ce-cuda, with NP or not

9-4-2018:
Possible gpu-daligner implementations:
- calculate bitmatrix on gpu, find snakes on cpu with clz/ctz
-- multiple encoding schemes are possible, preferably diagonally based
- calculate snakes on gpu, put lenghts in 2d-matrix, cpu will read snake instead of calculating
-- either encode all snakes (1 byte per snake), or use position->length mapping function (probably not feasible)

10-4-2018:
Mind signedness when shifting right!

12-4-2018:
Darwin has a new paper: 120x speedup over Daligner, expected 700x speedup for whole human assembly.
Their local alignment algorithm is Tiled Smith Waterman:
- for reverse wave:
- from a seed, create a tile of size TxT, where the right-bottom corner is the seed
- perform Smith Waterman on the whole tile, they have a systolic array, a GPU would need big tiles to allow the whole warp to be active
- find the best boundary cell (boundary left or up), this becomes the right-bottom corner for the next tile

13-4-2018:
Options:
- calc match/mismatch matrix on GPU on uncompressed bases, use simple snakefinding on CPU
- implement Tiled Smith-Waterman (Darwin GACT)

16-4-2018:
Naive GPU_SINGLE (one match/mismatch per byte, idx = apos+bpos*alen) has 12% cache misrate, 5% L1 data misrate
Problems witn GPU_SNAKE: the CPU can start in the middle of a snake, these points are not written by the GPU. If they would, they would not be coalesced at all. Maybe the CPU can detect if an apos/bpos combination is the start of a snake by checking apos-1 and bpos-1.
Started with TSW and CTSW ((CPU) Tiled Smith Waterman)

17-4-2018:
Linear gap penalty SW uses H(i,j) = max(A,B,C,D)
Affine gap penalty SW uses I(i,j) = max(A,B); D(i,j) = max(C,D); H(i,j) = max(0,I,D,E);

18-4-2018:
The reported apos/bpos is at the end of the first kmer (assuming an LAcall has 3 kmers)
For SW: either report the SW score, or calculate the number of diffs during traceback

19-4-2018:
SW stop condition: score = 0, or end of read reached. Different gap penalties and substitution matrix (S) values need to allow the score to come to 0 in time. Or calc number of diffs during traceback.

20-4-2018:
DOPA only returns the max score, not the traceback, this saves memory and time.
We might be able to use the score to keep track of the match/mismatch ratio, this means only the current and previous antidiagonals need to be kept in memory, the ones before that are not needed.
Talked to Zaid:
- find Darwin (TSW) CPU implementation
- accelerate their TSW on GPU (possibly with DOPA copy-paste)
- compare CPU/GPU TSW with CPU/GPU Daligner
- talk to Nauman for accuracy measurements

23-4-2018:
Talked to Nauman:
- measure accuracy of heuristics by comparing SW score of exact SW
- use simple scoring scheme for exact and heuristics (match ? 1 : -1)

25-4-2-2018:
Received GACT CPU code from Yatish.
Be careful with ILP when rewriting GACT CPU parts.
Added options GACT and FULL_GACT, GACT extends on two sides, FULL_GACT performs GACT on the whole matrix.

26-4-2018:
Daligner base representation: ACGT = [0-3], same for GACT
Talked to Zaid and Nauman:
- Only report start/endpoints and score, not traceback
- Possibly keep checkpoints with each tile
- Copy/paste from GASAL library
- View GACT as general SW algorithm, not specific to Darwin
- Accelerate GACT on GPU!
- Using linear gap might reduce memory requirement

14-5-2018:
Downloaded GASAL library, created branch gpu-daligner/GASAL
Implemented GASAL based alignment partially: passed LAcalls and Readpair info

15-5-2018:
Consideration when sending reads to GPU:
- produce mapping/unmapping of IDs, send all of them that have an LAcall, is very space expensive, unless it is done in parts
- rewrite buffer for reads each time, causes redundant writes, because a read usually has multiple LAcalls
- rewrite buffer for reads only when space can be saved, 
- keep track of filled and empty spots in buffer, to minimize redundant writing, needs some sort of monitor and causes external fragmentation of the buffer
GASAL uses 4 bits per packed base, 130MB has 133 million bases -> 67 million bytes, therefore we pick the first sending option.
GASAL packing also interleaves the bases, so prepacking all reads is not a good option.
Since different rpairs can have tiles in different directions and LAcalls start at different positions, uncoalesced accesses might occur. This could be countered by rewriting bases and relaunching kernels for each tile, but this will increase the CPU-GPU overhead.
Since uncoalesced access is annoying, and we have multiple CPU pthreads, we choose to relaunch for each tile.
Speaking about multiple CPU pthreads, GASAL functions are not asynchronous, so this will have to change too.

16-5-2018:
Expanded Rpair_status to contain abpos and LAcall_idx to allow tiling
Odd value: a_out[j] can be larger than tile_a_len at the end of a read. Efforts to fix using different padding characters and defining N_PENALTY were fruitless.

17-5-2018:
Started with changing the local GASAL kernel to fit GACT
Talked to Nauman:
- found bug in GASAL global first row score init
- put Overlap by Overlap matrix of scores in gmem, because you want to know the score of the aligned part without overlap. Normal overlap size is 120, but smem is too small for that

20-5-2018:
Overlap is not wrt elem with max score, but to next Tile.

22-5-2018:
Imagine three zones in the Tile:
- 1: terminate: when elem with max score has x < T/2 and y < T/2 (empiric)
- 2: easy: when elem with max score has x < T-O and y < T-O and not in terminate, new tile will start at that elem
- 3: hard: when elem with max score has x >= T-O or y >= T-O, easy new tile computation will not have enough overlap between tiles. But you cannot just to minus O or whatever, because you want to know where the trace hit the x = T-O or y = T-O line. To this end, perform this pseudocode:
if(x == T-O || y == T-O){
	start = 1;
}elif(x > T-O || y > T-O){
	propagate = 1;
}
i = x*T+y;
D = max(A,B,C);
if(A == D){
	if(start == 1){
		p[i] = ((x-1) << 8) | (y - 1);
	}elif(propagate == 1){
		p[i] = p[(x-1)*T + y - 1];
	}
}elif(B == D){
	if(start == 1){
		p[i] = ((x-1) << 8) | y;
	}elif(propagate == 1){
		p[i] = p[(x-1)*T + y];
	}
}elif(C == D){
	if(start == 1){
		p[i] = (x << 8) | (y - 1);
	}elif(propagate == 1){
		p[i] = p[x*T + y - 1];
	}
}
--------- rewritten
if(x == T-O || y == T-O){
	start = 1;
}elif(x > T-O || y > T-O){
	propagate = 1;
}
i = x*T+y;
D = max(A,B,C);
if(A == D){
	x--;
	y--;
}elif(B == D){
	x--;
}elif(C == D){
	y--;
}
if(start == 1){
	p[i] = (x << 8) | y;
}elif(propagate == 1){
	p[i] = p[x*T+y];
}

24-5-2018:
If part of the Tile is outside the DP matrix, there are different actions/calculations to terminate or find the start of a new Tile.

25-5-2018:
Talked to Zaid:
- focus on putting algorithm on GPU, not on changing algorithm
- put GASAL in CPU version of GACT

5-6-2018:
Finished moving GACT to GPU, same output as CPU version

7-6-2018:
Different max calculation of h_matrix_wr[j] is 13% faster.

8-6-2018:
Different max calculation of m_matrix_wr[j] is 0-9% faster.

11-6-2018:
Different max_score tracking if 4% faster, 0ac9c7b.
Coalesced all device-side matrices, output ok, 3x speedup, 8de15c5
Minor improvement with 'tmp += (ins_open >= ins_extend) ? (2 << INSERT_OP) : 0;', output ok, 7% faster, commit 2991155

12-6-2018:
Not checking to write pos_score each element, output ok, 0% speedup, 9581e87
Coalescing bases from ref_seq and query_seq, output ok, 0-13% speedup, f7ebc6f

15-6-2018:
Measured difference between matrixfill and traceback on GPU, matrixfill is about >99% of time, commit 798edc3
Changed generate.c to create datasets for gact/, because it needs no PacBio headers, and single lines

18-6-2018:
Talked to Zaid:
- profile matrixfill vs traceback on cpu
- thesis writing takes 1-2 months
- measure speedup on other datasets

19-6-2018:
Comparing the chars instead of using NtChar2Int(), 0-15% speedup (linear with tile_size), commit 7f5a691

22-6-2018:
Create type 5 reads, same as type 4, but they start at the same point of the genome to allow for simple gact benchmarking, as gact without filter has a constant start_pos.
TIME compile option shows: 100% in Align_Batch() for BATCH, 90% in Align_Batch_GPU() for BATCH GPU CMAT CBASES

25-6-2018:
Received D-SOFT code, is to be merged with GACT code.
The resulting GACT_calls from D-SOFT are sorted on query_id, then on query_pos. This means they would have to be sorted to create Readpairs with consecutive GACT_calls. We'll perform all GACT_calls then, instead of trying to skip some.
Default params:
match = 1
mismatch = gap_open = gap_extend = -1
seed_length = 13
tile_size = 320
tile_overlap = 120

29-6-2018:
Bug when using multiple CPU threads: GPU_storage struct was shared among all CPU threads
Got 10x speedup on the provided yeast data.
Talked to Nauman

2-7-2018:
Weird errors when using generated cat 4 data and BATCH vs CPU.
Caused by starting a GACT_call with pos==0, they stayed in reverse mode too long, fixed by adding another turnaround in the 'next_call==1' code.
Another bug for GACT_calls with pos==0: when initially assigned to a BATCH thread, they got wrong pos values during the turnaround, fixed by setting 'bpos=pos' during creation of the GACT_call.

3-7-2018:
Implemented CUDA streams for CPU multithreading.
gact.h:GPU_storage has a pointer to a CUDA_Stream_Holder, which is implemented in cuda_header.h, uses Forward Declaration to compile
Also created GPU_close() to destroy the streams.
Verified output when using streams.

4-7-2018:
Not enough VRAM (12GB) when using CBT 4 64 128, 3 64 128 works.
With 32 registers, occupancy increases to 46%, but runtime stays the same

5-7-2018:
75% of stalls are caused by memory depencies, more threads could solve this, but each thread needs tons of memory for the matrices.

6-7-2018:
Moved all scores to reference_guided.cpp, it will pass them to other functions, 8c39e13
Changed dir_matrix type from int to char to save space, output ok, 16b42b2

9-7-2018:
COMPRESS_DIR implemented, dir_matrix elements take 4 bits, but runtime is longer, due to more complex storing/reading, 6dfd022
Reverse reads on CPU, instead of GPU, eee7add
Load ref_nt once per column, instead of each matrix element, c0aa55a
Adding #pragma unroll 8 seems to have little effect
Talked to Zaid:
- balance workload between CPU and GPU
- check scalability of CPU for accelerated version
- implement GASAL to push GPU performance

10-7-2018:
Talked to Nauman:
- reported speedups
- reported possible GASAL local error

11-7-2018:
A problem with GASAL is the order in which the matrix elements are done, when two elements with the same highscore are calculated, only the first one is recorded. This could differ among the implementations. Chosen is to take the one with the highest ref_pos, then query_pos.

13-7-2018:
GPU and GPU GASAL have the same output now, but it is different from the CPU output. Some things with 0-based and 1-based indexing caused some +1 and -1 to change the outputs. cdd8a84
Initial tests show a 5x speedup for GPU GASAL vs GPU.
Printing bases only works when they are not coded to 0,1,2,3.

23-7-2018:
CPU and GPU GASAL have same output, errors for larger blocknumbers. Caused by i=-1 underflow (out of bounds) access for dir_matrix.

24-7-2018:
Fixed i=-1 underflow by adding extra elements for i=-1 and j=-1, ccbe9504b. Alternate fix is to check for negative values while tracebacking, but this check needs to be done for every iteration.

25-7-2018:
GASAL has 75x speedup vs CPU8 for 50MB dataset.

28-7-2018:
The basic traceback does not work when using an affine gap penalty. The old implementation must be used.

3-8-2018:
The basic traceback does not work for affine, because you need more info. Old implementation put back now.
Tried to divide converting bases to 2b values among CPU threads, failed because std::vector is not threadsafe. Fix: create converted strings instead of writing each base to the vector.
New problem: some threads start aligning before other threads are done with their base converting. Possible fix: use semaphores, not implemented in c++

4-7-2018:
Balanced base conversion using a condition_variable for synchronization, 581920c, saves about 1.7s for 50MB.

5-8-2018:
Coalescing h,f,p,d and global matrices seems to have a 5x slowdown, 712ab14, a possible reason is that the arrays are optimized to be in registers.
Using 128 registers seems to have a 7% speedup.

8-8-2018:
Only coalescing the 'global' matrix instead of also the h,f,p,d is slightly slower (5%).

27-8-2018:
Minimizers can be used, but they hurt sensitivity.

28-8-2018:
Minimizers can sometimes give seeds with ref_pos > ref_length.

29-8-2018:
GPU GASAL STREAM 64 CMAT: matfill 96% of time, traceback 4%.
Nauman: just use number of time SW (GACT) was started as sensitivity.
Downloaded NPBSS PacBio read generator.

30-8-2018:
Profiling shows that accessing packed_query_batch and packed_target_batch are very uncoalesced, as well as the out[] matrix.

3-9-2018:
Implemented COALESCE_PACKED_BASES, which interleaves bases on the CPU. The GPU packing kernel reads and writes coalesced, and the align kernel reads coalesced.
Initial run shows no speedup.

4-9-2018:
6.8MB, 1 64 64:
normal:  total_GPU_prep: 171 ms, total_pack:  8.4 ms, total_align: 6342 ms
CPBASES: total_GPU_prep: 388 ms, total_pack: 23.3 ms, total_align: 6536 ms
50MB, 1 256 64:
normal:  total_GPU_prep: 4172 ms, total_pack: 53.6 ms, total_align: 41.1 s
CPBASES: total_GPU_prep: 6002 ms, total_pack: 95.7 ms, total_align: 38.9 s

Talked to Zaid:
- benchmark everything

NPBSS creates the same simulated reads everytime, because the matlab rng is initialized in the same way at startup.

5-9-2018:
Changed NPBSS/simulatePacBio4.p to simulatePacBio4.m, same output
Added the origin of the read in the ref (position) to the output readname.

10-9-2018:
How to measure sensitivity:
- generate two sets of reads from genome, store original position
- align them with aligner, check if overlaps coincide with original positions
For 50MB, 2% of reported overlaps are duplicates, for small NPBSS: 15-20%

13-9-2018:
Online EMBOSS water tool uses match:5, mismatch:-4 as default for DNA alignment.
ksw uses gapo+gape as gapo, so compensate.
ksw has problems printing the name of the subread (with coordinates in the name) when the score is low, (lower than 4)

17-9-2018:
Downloaded human reference from https://www.gencodegenes.org/releases/21.html, FASTA region ALL, might actually not be needed at all. The E.coli reference in NPBSS should be enough to measure sensitivity.

18-9-2018:
Changed NPBSS default error rate to 9% ins, 4.5% del, 1.5% sub, Darwin metrics still bad.
Downloaded PBSIM

20-9-2018:
Talked to Yatish:
- don't actually do exact SW, only compare to theoretical ovls

24-9-2018:
Great sensitivity for Darwin ref: 99.9%, spec: 98.5%
Great sensitivity for Daligner ref: 98.3%, spec: 97.4%
Terrible sensitivity for Darwin denovo: 45%, spec: 66%
Mediocre sensitivity for Daligner denovo: 85.6%, spec: 86.5%

25-9-2018:
Talked to Zaid:
- create 0% error reads, heuristic should have 100% sensitivity
- ask Yatish, results should be easily reproducible

27-9-2018:
older defaults:
h = 26
num_seeds = 1000

28-9-2018:
Talked to Zaid:
- measure runtime vs sensitivity

1-10-2018:
Set default PBSIM seed to 1538399012

3-10-2018:
Using the same seed for PBSIM to generate two datasets resulted in two identical datasets.
The default PBSIM seed for the second dataset is 1156173320

16-10-2018:
When using TIME, it looks like 90% of the time_gpu (time spent in cuda_host) is taken by the gasal_local_kernel()

8-11-2018:
Combined github/darwin with own Darwin branches, includes minimizers, crashes due to 'out_of_range' for substr()
Caused by minimizers producing seeds with ref_pos > ref_len, this leads to negative tile_sizes during forward extension. Fixed by setting ref_pos = ref_len in that case, 7d264c63ef

9-11-2018:
Higher window_size lead to higher sensitivity and lower runtime!
Highest sensitivity for 0-error reads is 99.9982%, sensitivity increases for higher window sizes. For 15%-error reads, this optimum lies at w=4.
Highest window_size is 13, for Kmer_size = 14.

12-11-2018:
Redoing benchmarks for GPU without GASAL is very difficult.
We want: GPU and GPU CMAT, to show the impact of CMAT, later show CBASES too.
Then we show that GASAL is even faster than GPU CMAT CBASES

19-11-2018:
Moved ref_pos > ref_len fix to CPU as well.
Removed printing of trivial alignments, because CPU and BATCH differs, now output ok, eacc139






TODO: create type 6 reads that have a AT/GC imbalance.
	  create type 7 reads that are a subset from real reads, but with nice lengths, etc.

./generate 4 200 20 440 1      has 1 rpair ab: 66
./generate 4 1000 161 1200 1   has 128 rpairs
./generate 4 300 1 400 1       has 1 rpair
./generate 4 100 100 150 1     has 1 rpair
./generate 4 1000 44 1200 1    has 32 rpairs
./generate 4 3000 32 5000 1    has 32 rpairs
./generate 4 3000 33096 5000 1 has 32768 rpairs

Add asm("exit;") when ERROR CANNOT REALLOC TRACE CELLS occurs


TODO: find out relation between number of seed hits, etc. and overlaps. This can be used to preprocess the readpairs and possibly decrease divergence. First check if the threads really have a lot of divergence.


9-8-2017
cuda_device.cu draft:
while(1){
	if(cpair == done (i > cpair.eidx)){
		getNewReadpair();
	}
	while(c->apos < lasta[diag]){
		try next seed hit
	}
OR
	while(not ready for LAcall){
		if(cpair == done (i > cpair.eidx)){
			getNewReadpair();
		}
		if(c->apos < lasta[diag]){
			break;
		}
		try next seed hit
	}


	LAcall()
	checkOverlap()
	if(cpair == done){
		HandleRedundancies()
	}
}


typedef struct{             // size(LAcall) == 168
  Alignment  align;
  Overlap    ovla, ovlb;
  int        diag, adiag;
  int        doA, doB;      // could be compressed into one byte
  int        aread;       // keep track of which read pair this LAcall belongs to
  int        bread;         // NOT NEEDED, info is also in ovla and ovlb
  Path      *bpath;
} LAcall;

struct Alignment{	// sizeof(Alignment) = 40
	Path *path;		// points to apath/ovla->path
	uint32 flags;	// this either 0 or 1 for complement DB
	char *aseq;		// pointer to aseq
	char *bseq;		// pointer to bseq
	int alen;
	int blen;
}

struct Path{		// sizeof(Path) = 32
	void *trace;	// vector of trace cells, actually points to work->points
	int tlen;
	int diffs;
	int abpos, bbpos;
	int aepos, bepos;
}

struct Overlap{		// sizeof(Overlap) = 48
	Path path;
	uint32 flags;
	int aread;		// index of read (ar + afirst(constant))
	int bread;
}

typedef void Work_Data;

struct _Work_Data{
	int vecmax;
	void *vector;	// contains V,M,NA,NB,HA,HB, sizeof(VectorEl) = 32
	int celmax;
	void *cells;	// actually holds Pebbles, sizeof(Pebble) = 16
	int pntmax;
	void *points;	// actually holds trace cells
	int tramax;
	void *trace;	// not used unless complete alignment is needed
}
