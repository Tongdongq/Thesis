\documentclass[../main/thesis.tex]{subfiles}

\begin{document}

\chapter{Introduction}
\ifdefined\main
\else
\input{../notmain.tex}
\fi

The processes in biology are extremely complex, especially those in multicellular organisms.
Pretty much all of these processes are governed by DNA, the building blocks of life.
Our kind has spent ages trying to unravel its secrets.
Literally, because we found out that DNA consists of a double helix, which must be unwound to be used.

In 2012, we discovered that the human genome consists of about three billion basepairs, wrapped up in 23 chromosomepairs \cite{human_genome_project}.
And that it shares 60\% with the genome of a banana \cite{banana}.
However, DNA still has many unexposed secrets.
For example, some diseases are partially caused by genes, like breast cancer or sickle cell disease \cite{genomic_diseases}.

To gain more knowledge about the influence of DNA and genes in general, they must be identified.
This is where DNA sequencing comes in.
With a blood sample, or some saliva, the DNA of an individual can be mapped.
Combining the DNA of many people can teach us what the effect is of some genes.
However, DNA sequencing machines are not perfect, they only provide small parts of the DNA, and they contain errors too.
These small parts must be assembled into a whole genome.
One phase of this assembly is the alignment part, where overlaps between those small parts are found.
Performing the normal, exact way of alignment using Smith-Waterman is not feasible, so many algorithms have been developed to perform this alignment, for different lengths and error rates.
Seed-and-extend is one heuristic, which dramatically reduces the amount of computation needed.
Instead of performing Smith-Waterman on each readpair, only readpairs with promising regions are considered.
These promising regions are called seeds, and are found using Kmers.
A Kmer is a piece of K consecutive nucleotides.
Many different techniques exist to split and store the reads into Kmers.
The reads are compared to eachother, reads that have enough matching Kmers in a small enough region, are extended using various methods.
Another approach is a deBruijn graph, each Kmer is a vertex, and an edge is created for each next Kmer.
This results in a large graph with up to $4^K$ vertices, and even more edges, a normal value for K is 21.
The resulting assembly is found by calculating the Hamiltonian Path \cite{Euler_Hamil_paths}.
So called Next Generation Sequencing (NGS) techniques produce reads with lengths anywhere from 50 to 700.
They can be produced very quickly, but a drawback is their length.
DNA can contain repeat regions, where a certain piece of DNA is repeated many times back-to-back, or a repeat could appear in many different places in the genome.
These repeats can be longer than the produced reads, which means the reads cannot be used to resolve these repeats.

Third generation sequencing produce much longer reads, of up to 60000 basepairs.
Due to their length, they are more likely to contain a whole repeat section, which means it can be used to connect the pieces before and after the repeat.
A major drawback is that they also contain more errors, 15-30\%, depending on the exact technology.

Daligner and Darwin are two algorithms that find overlaps for so-called 'long reads'.
They are heuristics, that reduce the amount of computation needed, without compromising the output much.
Still, they do a lot of computation, Daligner takes 15,600 CPU hours to align a Human genome with 54x oversampling.

Since DNA alignment is inherently parallel (comparing reads A and B does not depend on the result of comparing reads C and D, or even A and C), speedup can be gained by parallelising the workload.
GPUs are built for graphics applications, but modern, programmable GPUs allow for GPGPU (General-Purpose computing on GPUs).
This means that any algorithm can be run on a GPU, but implementing them is not a trivial task.
The GPU must be explicitly told what to do, and using the GPU also introduces communication overhead.
Moreover, GPU cores use a simpler instruction set than CPU cores, and run on lower clocks.
To gain speedup with a GPU, the workload must be divided well, to ensure the available hardware is sufficiently utilised.

In this work, Daligner and Darwin are implemented to run on a GPU, in particular a Tesla K40.

\paragraph{Outline}
Chapter 2 Background contains details about the working of DNA in biology, as well as DNA sequencing and assembly techniques.
Chapter 3 Concept further explains the algorithms of Daligner and Darwin.
Chapter 4 Specification contains measured statistics that help understand the working of the algorithm, as well as all the implementations/optimizations done on both algorithms.
The evaluation of these implementations is presented in Chapter 5 Results.
In Chapter 6 Conclusion, conclusions and recommendations for future work are presented.


\end{document}








